\documentclass[11pt]{article}

% This first part of the file is called the PREAMBLE. It includes
% customizations and command definitions. The preamble is everything
% between \documentclass and \begin{document}.

\usepackage[margin=0.6in]{geometry} % set the margins to 1in on all sides
\usepackage{graphicx} % to include figures
\usepackage{amsmath} % great math stuff
\usepackage{amsfonts} % for blackboard bold, etc
\usepackage{amsthm} % better theorem environments
% various theorems, numbered by section
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{array}
\usepackage{courier}
\usepackage[usenames, dvipsnames]{color}
\usepackage{titlesec}
\usepackage{empheq}
\usepackage{tikz}
% \usepackage{pslatex}


\newcommand\encircle[1]{%
  \tikz[baseline=(X.base)] 
    \node (X) [draw, shape=circle, inner sep=0] {\strut #1};}
 
% Command "alignedbox{}{}" for a box within an align environment
% Source: http://www.latex-community.org/forum/viewtopic.php?f=46&t=8144
\newlength\dlf  % Define a new measure, dlf
\newcommand\alignedbox[2]{
% Argument #1 = before & if there were no box (lhs)
% Argument #2 = after & if there were no box (rhs)
&  % Alignment sign of the line
{
\settowidth\dlf{$\displaystyle #1$}  
    % The width of \dlf is the width of the lhs, with a displaystyle font
\addtolength\dlf{\fboxsep+\fboxrule}  
    % Add to it the distance to the box, and the width of the line of the box
\hspace{-\dlf}  
    % Move everything dlf units to the left, so that & #1 #2 is aligned under #1 & #2
\boxed{#1 #2}
    % Put a box around lhs and rhs
}
}


\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}

\setcounter{secnumdepth}{4}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\definecolor{myblue}{RGB}{72, 165, 226}
\definecolor{myorange}{RGB}{222, 141, 8}

\setlength{\heavyrulewidth}{1.5pt}
\setlength{\abovetopsep}{4pt}


\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\argmin}{\arg\!\min}
\DeclareMathOperator{\Tr}{Tr}

\newcommand{\bd}[1]{\mathbf{#1}} % for bolding symbols
\newcommand{\RR}{\mathbb{R}} % for Real numbers
\newcommand{\ZZ}{\mathbb{Z}} % for Integers
\newcommand{\col}[1]{\left[\begin{matrix} #1 \end{matrix} \right]}
\newcommand{\comb}[2]{\binom{#1^2 + #2^2}{#1+#2}}
\newcommand{\bs}{\boldsymbol}
\newcommand{\opn}{\operatorname}
\begin{document}
\nocite{*}
% \fontfamily{ptm}
% \renewcommand\rmdefault{ptm}

\title{Rosen's paper review}

\author{Daeyoung Lim\thanks{Prof. Taeryon Choi} \\
Department of Statistics \\
Korea University}

\maketitle
\section{Model Specifications}
  \begin{equation}
    \bs{y}_{i}\left(t_{ij}\right) = \bs{X}_{ij}\bs{\mu}\left(t_{ij}\right) + \bs{Z}_{ij}\bs{g}_{i}\left(t_{ij}\right) + \bs{\delta}_{i}\left(t_{ij}\right)
  \end{equation}
  where
  \begin{align}
    \bs{y}_{i}\left(t_{ij}\right)&:\; p\times 1 \qquad i=1,\ldots, n, \; j = 1,\ldots, m_{i}\\
    \bs{\mu}\left(t\right) &= \left(\bs{\mu}'_{1}\left(t\right), \ldots , \bs{\mu}'_{p}\left(t\right)\right)'\\
    \bs{\mu}'_{k}\left(t\right) &= \left(\mu_{k1}\left(t\right),\ldots , \mu_{kr}\left(t\right) \right)' : &r\times 1\\
    \bs{g}_{i}\left(t\right) &= \left(\bs{g}'_{i1}\left(t\right),\ldots , \bs{g}'_{ip}\left(t\right)\right)'\\
    \bs{g}_{ik}\left(t\right) &= \left(g_{ij1}\left(t\right),\ldots, g_{iks}\left(t\right)\right)':  &s \times 1\\
    \bs{x}_{ij}&:\; r \times 1\\
    \bs{z}_{ij}&:\; s\times 1\\
    \bs{X}_{ij} &= \bs{I}_{p} \otimes \bs{x}_{ij}'\\
    \bs{Z}_{ij} &= \bs{I}_{p} \otimes \bs{z}_{ij}'\\
    \bs{\delta}_{i}\left(t_{ij}\right) &\sim \text{Ornstein-Uhlenbeck process}
  \end{align}
\section{Ornstein-Uhlenbeck process}
An Ornstein-Uhlenbeck process is the second-order stationary process $\left\{X_{t}\right\}$, that satisfies the following differential equation:
\begin{equation}
  dX\left(t\right) = -aX\left(t\right)\,dt + \sigma\,dB\left(t\right), \quad t\geq 0
\end{equation}
where $\left\{B\left(t\right)\right\}$ is standard Brownian motion, and $a$ and $\sigma > 0$ are parameters and $X_{0}$ is a random variable that is independent of $\left\{B\left(t\right) \right\}$.
\subsection{Multivariate Ornstien-Uhlenbeck process}
The univariate version of the OU process naturally evolves to a multivariate form which reads as follows:
\begin{equation}
  d\bs{X}\left(t\right) = -\bs{A}\bs{X}\left(t\right)\,dt + \bs{B}\,d\bs{W}\left(t\right),
\end{equation}
where $\bs{A}, \bs{B}$ are constant matrices. The solution for this SDE is
\begin{equation}
  \bs{X}\left(t\right) = \exp\left(-\bs{A}t\right)\bs{X}_{0} + \int_{0}^{t}\exp\left\{-\bs{A}\left(t-t'\right) \right\}\bs{B}\,d\bs{W}\left(t\right).
\end{equation}
The properties of such an OU process are as follows:
\begin{itemize}
  \item If $\bs{A}$ has only eigenvalues with positive real part, a stationary solution exists of the form
  \begin{equation}
    \bs{X}_{s}\left(t\right) = \int_{-\infty}^{t}\exp\left\{-\bs{A}\left(t-t'\right) \right\}\bs{B}\,d\bs{W}\left(t\right).
  \end{equation}
  The expected value $\opn{E}\left[\bs{X}_{s}\left(t\right)\right] = 0$ and the covariance matrix
  \begin{equation}
    \bs{\Sigma} = \opn{Cov}\left(\bs{X}_{s}\left(t\right), \bs{X}_{s}'\left(s\right)\right) = \int_{-\infty}^{\min\left(t,s\right)}\exp\left\{-\bs{A}\left(t-t'\right) \right\}\bs{BB}'\exp\left\{-\bs{A}'\left(s-t'\right) \right\}\,dt'.
  \end{equation}
  \item The stationary covariance matrix satisfies the following equation:
  \begin{equation}
    \bs{A\Sigma} + \bs{\Sigma A}' = \bs{BB}'.
  \end{equation}
  The solution to this equation is given by
  \begin{equation}
    \bs{\Sigma} = \frac{\left|\bs{A}\right|\bs{BB}' + \left[\bs{A}-\Tr\left(\bs{A}\right)\bs{I} \right]\bs{BB}'\left[\bs{A}-\Tr\left(\bs{A}\right)\bs{I}\right]}{2\left(\Tr\left(\bs{A}\right)\right)\left|\bs{A}\right|}
  \end{equation}
  \item \emph{(Time Correlation Matrix in the Stationary State)} The following relations hold.
  \begin{align}
    \opn{Cov}\left(\bs{X}_{s}\left(t\right), \bs{X}'_{s}\left(s\right)\right) &= \exp\left\{-\bs{A}\left(t-s\right) \right\}\bs{\Sigma}, & t>s \\
    &= \bs{\Sigma}\exp\left\{-\bs{A}'\left(s-t\right) \right\}, & t<s
  \end{align}
  \item If all its transition densities depend only on the time differences, then the Markov process is \emph{homogeneous}. OU process is homogeneous. Therefore, the transition probabiliy of an OU process is given by
  \begin{equation}
    \opn{P}\left(\bs{X}_{s}\left(t\right)|\bs{X}_{s}\left(s\right), t-s\right) = \left|2\pi\bs{\Omega}\right|^{-1/2}\exp\left\{-\frac{1}{2}\bs{\gamma}'\bs{\Omega}^{-1}\bs{\gamma} \right\}
  \end{equation}
  where
  \begin{align}
    \bs{\gamma} &= \bs{X}_{s}\left(t\right) -\exp\left(-\bs{A}\left(t-s\right)\right)\bs{X}_{s}\left(s\right) \\
    \bs{\Omega} &=  \bs{\Sigma} -\exp\left\{-\bs{A}\left(t-s\right) \right\}\bs{\Sigma}\exp\left\{-\bs{A}'\left(t-s\right)\right\}.
  \end{align}
\end{itemize}
\section{Cubic Spline Interpolation}
When the data come in the form of pairs $\left(x_{i},y_{i}\right)$, it is often our interest to find out what underlying \emph{function} they fall onto. There are many ways of achieving such a goal but one of the most popular methods is called \emph{the cubic spline}. It is a nonparametric way of intepolation guaranteeing the continuous twice-differentiability. Unlike linear interpolation, the cubic spline is differentiable even at the end point of each interval. Let's pick an arbitrary interval between $x_{j}$ and $x_{j+1}$. Following the notation of the book \emph{Numerical Recipes in C}, $y_{i} = y\left(x_{i}\right),\; i = 1,\ldots , N$. It is proven that the function can be uniquely constructed as
\begin{equation}
  y = Ay_{j} + By_{j+1} + Cy''_{j} + Dy''_{j+1}
\end{equation}
where $A,B,C,D$ are
\begin{align}
  &A \equiv \frac{x_{j+1}-x}{x_{j+1}-x_{j}} &B \equiv \frac{x-x_{j}}{x_{j+1}-x_{j}}\\
  &C \equiv \frac{1}{6}\left(A^{3}-A\right)\left(x_{j+1}-x_{j}\right)^{2} &D \equiv \frac{1}{6}\left(B^{3}-B\right)\left(x_{j+1}-x_{j}\right)^{2}.
\end{align}
The first and second derivatives of $y$ with respect to $x$ are given by
\begin{align}
  \frac{dy}{dx} &= \frac{y_{j+1}-y_{j}}{x_{j+1}-x_{j}} - \frac{3A^{2}-1}{6}\left(x_{j+1}-x_{j}\right)y''_{j}+\frac{3B^{2}-1}{6}\left(x_{j+1}-x_{j}\right)y''_{j+1}\\
  \frac{d^{2}y}{dx^{2}} &= Ay''_{j} + By''_{j+1}.
 \end{align}
 \subsection{Technical details regarding cubic splines}
 Accepting that the above equations are given, the only problem left for us is that we actually do not know $y''_{i}$ yet assumed them to be known when modeling. It is also given for $j=2, \ldots , N-1$
 \begin{equation}
  \frac{x_{j}-x_{j-1}}{6}y''_{j-1} + \frac{x_{j+1}-x_{j-1}}{3}y''_{j} + \frac{x_{j+1}-x_{j}}{6}y''_{j+1} = \frac{y_{j+1}-y_{j}}{x_{j+1}-x_{j}} - \frac{y_{j}-y_{j-1}}{x_{j}-x_{j-1}}.
 \end{equation}
 In order to uniquely determine the curve, 2 more conditions need to be made. The most common ways of doing this are
 \begin{itemize}
  \item set one or both of $y''_{1}$ and $y''_{N}$ equal to zero, giving the so-called \emph{natural cubic spline}, which has zero second derivative on one or both of its boundaries, or
  \item set either of $y''_{1}$ and $y''_{N}$ to values calculated from equation (27) so as to make the first derivative of the interpolating function have a specified value on either or both boundaries.
 \end{itemize}
Part of cubic splines' popularity is due to the linearity of the equations as well as the tridiagonality. In particular, the tridiagonality makes the computation so much easier, enabling it to be solved in $O\left(N\right)$ operations instead of $O\left(N^{3}\right)$ for the general linear problem. Refer to \emph{tridiagonal matrix algorithm} or equivalently \emph{Thomas algorithm} for further details.
\subsection{Cubic spline in the paper}
Cubic splines can be rewritten as a specific case of B-splines. In the paper, the author constructs a general setting where the model is
\begin{equation}
  y_{i} = f\left(x_{i}\right) + \epsilon_{i},
\end{equation}
where $\opn{E}\left(\epsilon_{i}\right) = 0$ and $f$ is an unknown smooth function. Upon choosing the knots $\kappa_{1}, \ldots , \kappa_{K}$, the cubic spline is reexpressed as a linear combination of the basis functions $1, x, \left|x-\kappa_{1}\right|^{3}, \ldots , \left|x-\kappa_{K}\right|^{3}$:
\begin{equation}
  f\left(x\right) = \beta_{0} + \beta_{1}x + \sum_{k=1}^{K}u_{k}\left|x-\kappa_{k}\right|^{3}.
\end{equation}
It is, at all times, not a good idea to overload a notation for multiple different things but the author does not seem to care. Hence, again uses $\bs{\Omega}_{K}$ whose $\left(k,\ell\right)$th element is $\left|\kappa_{k}-\kappa_{\ell}\right|^{3}$. The author of the paper draws another representation of the fitting problem in the form of a mixed model by recasting the function in a matrix form and writing down the frequentist loss function. The loss function is
\begin{equation}
  L\left(\bs{y},\bs{x},\bs{\theta}\right) = \sum_{i=1}^{n}\left(y_{i}-f\left(x_{i}\right)\right)^{2} + \frac{1}{\lambda}\bs{\theta}'\bs{D\theta}
\end{equation}
where $\bs{\theta}= \left(\beta_{0},\beta_{1}, u_{1}, \ldots, u_{K}\right)'$ and the matrix $\bs{D}$ is
\begin{equation}
  \bs{D} = \begin{bmatrix}\bs{0}_{2\times 2} & \bs{0}_{2\times K}\\ \bs{0}_{K \times 2} & \bs{\Omega}_{K}  \end{bmatrix}.
\end{equation}
Now let's define $\bs{X} = \begin{bmatrix} \bs{1} & \bs{x}\end{bmatrix}$ and $\bs{Z}_{K} = \begin{bmatrix} \left|x_{i}-\kappa_{1}\right|^{3} & \ldots & \left|x_{i} - \kappa_{K}\right|^{3}\end{bmatrix}_{1\leq i \leq n}$. Now we can reformulate the equation (32):
\begin{equation}
  \frac{1}{\sigma_{\epsilon}^{2}}\left\|\bs{y}-\bs{X\beta}-\bs{Z}_{K}\bs{u}\right\|^{2} + \frac{1}{\lambda\sigma_{\epsilon}^{2}}\bs{u}'\bs{\Omega}_{K}\bs{u}
\end{equation}
where $\bs{\beta}=\left(\beta_{0},\beta_{1}\right)'$ and $\bs{u}=\left(u_{1},\ldots , u_{K}\right)'$. We have just converted the cubic spline fitting problem into an equivalent linear mixed model fitting problem with a common setting of
\begin{equation}
  \bs{y}=\bs{X\beta}+\bs{Z}_{K}\bs{u} + \bs{\epsilon}, \qquad \opn{Cov}\left(\bs{u}\right) = \sigma_{\bs{u}}^{2}\bs{\Omega}_{K}^{-1}.
\end{equation}
Furthermore, by defining $\bs{b}=\bs{\Omega}_{K}^{1/2}\bs{u}$ and $\bs{Z}=\bs{Z}_{K}\bs{\Omega}_{K}^{-1/2}$, the equation (35) is the same as
\begin{equation}
  \bs{y}=\bs{X\beta} + \bs{Zb}+\bs{\epsilon}, \qquad \opn{Cov}\begin{pmatrix}\bs{b}\\ \bs{\epsilon} \end{pmatrix}=\begin{pmatrix} \sigma_{\bs{b}}^{2}\bs{I}_{K} & \bs{0}\\ \bs{0} & \sigma_{\epsilon}^{2}\bs{I}_{n}\end{pmatrix}.
\end{equation}
\section{Priors and sampling}
Recall the original setting:
\begin{equation}
  \bs{y}_{i}\left(t_{ij}\right) = \bs{X}_{ij}\bs{\mu}\left(t_{ij}\right) + \bs{Z}_{ij}\bs{g}_{i}\left(t_{ij}\right) + \bs{\delta}_{i}\left(t_{ij}\right).
\end{equation}
Using the above cubic spline approach, we have two coefficient functions to estimate: $\bs{\mu}\left(t_{ij}\right)$ and $\bs{g}_{i}\left(t_{ij}\right)$. Thus, we will, as we have above, transform them into matrix multiplications:
\begin{align}
  \mu_{k\ell}\left(t_{ij}\right) &= \bs{\phi}'_{ij}\bs{\beta}_{k\ell} + \bs{\psi}'_{ij}\bs{v}_{k\ell}\\
  g_{ikm}\left(t_{ij}\right) &= \bs{\phi}'_{ij}\bs{w}_{ikm} + \bs{\psi}'_{ij}\bs{u}_{ikm}
\end{align}
where $k = 1,\ldots , p$, $\ell = 1, \ldots , r$, $m = 1,\ldots, s$, $i = 1,\ldots , n$ and $\kappa_{1},\ldots , \kappa_{K}$ are knots which are sample quantiles of $t_{ij}$ ($j=1,\ldots , m_{i}$). Let $\bs{\Lambda}_{K}$ take the place of $\bs{\Omega}_{K}$ in the previous section. Therefore,
\begin{equation}
  \bs{\Lambda}_{K} = \begin{bmatrix} \left| \kappa_{k}-\kappa_{\ell}\right|^{3}\end{bmatrix}_{1\leq k, k'\leq K}.
\end{equation}
And the other parameters are given by
\begin{align}
  \bs{\phi}'_{ij} &= \begin{bmatrix}1 & t_{ij} \end{bmatrix}\\
  \bs{\xi}'_{ij} &= \begin{bmatrix} \left|t_{ij}-\kappa_{1}\right|^{3} & \ldots  & \left|t_{ij}-\kappa_{K}\right|^{3}\end{bmatrix}\\
  \bs{\psi}'_{ij} &= \bs{\xi}'_{ij}\bs{\Lambda}_{K}^{-1/2}.
\end{align}
Then the priors are
\begin{align}
  \bs{\beta}_{k\ell} &\sim \mathcal{N}\left(\bs{0}, \sigma_{\bs{\beta}_{k\ell}}^{2}\bs{I}_{2}\right)\\
  \bs{v}_{k\ell} &\sim \mathcal{N}\left(\bs{0}, \sigma_{\bs{v}_{k\ell}}^{2}\bs{I}_{K}\right)\\
  \bs{w}_{ikm} &\sim \mathcal{N}\left(\bs{0}, \opn{diag}\left(\sigma_{\bs{w}_{km0}}^{2}, \sigma_{\bs{w}_{km1}}^{2}\right)\right)\\
  \bs{u}_{ikm} &\sim \mathcal{N}\left(\bs{0}, \sigma_{\bs{u}_{km}}^{2}\bs{I}_{K} \right)
\end{align}.
\subsection{OU process priors}
It is very unfamiliar for one to have OU process applied to a statistical model since we have thus far only considered Gaussian error terms. The paper states that the parameters that define an OU process are $\bs{A}$ and $\bs{BB}'$.
\begin{itemize}
  \item Eigendecompose $\bs{A}$ as $\bs{A} = \bs{S\Psi S}^{-1}$. $\bs{S} = \left(s_{ij}\right), \; i,j=1,\ldots , p$, with unit diagonal elements. The off-diagonal elemtns of $\bs{S}$ and the logarithm of the diagonal elements of $\bs{\Psi}$ follow independent $\mathcal{N}\left(0, \sigma_{a}^{2}\right)$.
  \item $\bs{BB}'$ is positive definite. Decompose into $\bs{C}=\bs{LDL}'$ where $\bs{L}$ is unit lower triagular and $\bs{D}$ is diagonal. The off-diagonal elements of $\bs{L}$ follow independent $\mathcal{N}\left(0,\sigma_{L}^{2}\right)$ and the logarithm of the diagonal elements of $\bs{D}$ follow independent $\mathcal{N}\left(0,\sigma_{D}^{2}\right)$.
\end{itemize}
\subsection{Sampling scheme}
\begin{align}
  \bs{\theta}_{k} &= \left(\bs{\beta}'_{k1}, \bs{v}'_{k1}, \ldots , \bs{\beta}'_{kr}, \bs{v}'_{kr}\right), \; k=1,\ldots , p\\
  \bs{\theta} &= \left(\bs{\theta}'_{1},\ldots , \bs{\theta}'_{p}\right)\\
  \bs{\eta}_{ik} &= \left(\bs{w}'_{ik1}, \bs{u}'_{ik1}, \ldots , \bs{w}'_{iks},\bs{u}'_{iks}\right), \; k=1,\ldots , p\\
  \bs{\eta} &= \left(\bs{\eta}'_{i1}, \ldots , \bs{\eta}'_{ip}\right), \; i=1,\ldots , n
\end{align}
Then equation (1) can be recast as
\begin{equation}
  \bs{y}_{i}\left(t_{ij}\right) = \bs{\chi}_{ij}\bs{\theta} + \bs{E}_{ij}\bs{\eta}_{i} + \bs{\delta}_{i}\left(t_{ij}\right)
\end{equation}
where $\bs{\chi}_{ij}=\bs{I}_{p} \otimes \left(\bs{x}'_{ij}\Gamma_{rij}\right)$, $\bs{E}_{ij}=\bs{I}_{p}\otimes \left(\bs{z}'_{ij}\Gamma_{sij}\right)$, $\Gamma_{rij}=\bs{I}_{r} \otimes \left(\bs{\phi}'_{ij}, \bs{\psi}'_{ij}\right)$, and $\Gamma_{sij}=\bs{I}_{s} \otimes \left(\bs{\phi}'_{ij}, \bs{\psi}'_{ij}\right)$.
\subsubsection{$\bs{\theta}$}
Combining the priors given in section 4, $\bs{\theta} \sim \mathcal{N}\left(\bs{0},\bs{G}^{-1}\right)$ where
\begin{equation}
  \bs{G}=\opn{blockdiag}\left(\sigma_{\beta_{k11}}^{-2}\bs{I}_{2},\sigma_{v_{11}}^{-2}\bs{I}_{K}, \ldots , \sigma_{\beta_{1r}^{-2}}\bs{I}_{2}, \sigma_{v_{1r}}^{-2}\bs{I}_{K}, \ldots , \sigma_{\beta_{p1}}^{-2}\bs{I}_{2}, \sigma_{v_{p1}}^{-2}\bs{I}_{K}, \ldots , \sigma_{\beta_{pr}}^{-2}\bs{I}_{2}, \sigma_{v_{pr}}^{-2}\bs{I}_{K}\right).
\end{equation}
Therefore, the full conditional of $\bs{\theta}$ is given by
\begin{equation}
  \opn{P}\left(\bs{\theta}|\text{rest}\right) \propto \prod_{i=1}^{n}\prod_{j=1}^{m_{i}}\exp\left\{-\frac{1}{2}\bs{\gamma}_{t_{ij}}'\bs{\Omega}_{\Delta t_{ij}}^{-1}\bs{\gamma}_{t_{ij}} \right\}\exp\left\{-\frac{1}{2}\bs{\theta}'G\bs{\theta} \right\}.
\end{equation}
We should convert $\bs{\gamma}_{t_{ij}}$ into a term including $\bs{y}_{i}\left(t_{ij}\right)$.
\begin{align}
  \bs{\gamma}_{t_{ij}} &= \bs{\delta}_{i}\left(t_{ij}\right) - \exp\left(-\bs{A}\Delta t_{ij}\right)\bs{\delta}_{i}\left(t_{i,j-1}\right)\\
  \bs{\delta}_{i}\left(t_{ij}\right) &= \bs{y}_{i}\left(t_{ij}\right) -\bs{\chi}_{ij}\bs{\theta} - \bs{E}_{ij}\bs{\eta}_{i}\\
  \bs{\delta}_{i}\left(t_{i,j-1}\right) &= \bs{y}_{i}\left(t_{i,j-1}\right) -\bs{\chi}_{i,j-1}\bs{\theta} - \bs{E}_{i,j-1}\bs{\eta}_{i}
\end{align}
Plugging in equation (56), (57) to (55) gives
\begin{equation}
  \bs{\gamma}_{t_{ij}} = \bs{y}_{i}\left(t_{ij}\right) - \bs{\chi}_{ij}\bs{\theta}-\bs{E}_{ij}\bs{\eta}_{i}-\exp\left(-\bs{A}\Delta t_{ij}\right)\left\{\bs{y}_{i}\left(t_{i,k-1}\right) -\bs{\chi}_{i,j-1}\bs{\theta}-\bs{E}_{i,j-1}\bs{\eta}_{i} \right\}.
\end{equation}
To separate the terms that include $\bs{\theta}$ from those that don't, we define
\begin{align}
  \bs{\zeta}_{i}\left(t_{ij}, t_{i,j-1}\right) &= \bs{y}_{i}\left(t_{ij}\right)-\bs{E}_{ij}\bs{\eta}_{i} - \exp\left(-\bs{A}\Delta t_{ij}\right)\left(\bs{y}_{i}\left(t_{i,j-1}\right)-\bs{E}_{i,j-1}\bs{\eta}_{i}\right)\\
  \bs{\chi}_{i}\left(t_{ij},t_{i,j-1}\right) &= \bs{\chi}_{ij} - \exp\left(-\bs{A}\Delta t_{ij}\right)\bs{\chi}_{i,j-1}.
\end{align}
Then, $\bs{\gamma}_{t_{ij}} = \bs{\zeta}_{i}\left(t_{ij}, t_{i,j-1}\right) - \bs{\chi}_{i}\left(t_{ij},t_{i,j-1}\right)\bs{\theta}$. Plugging this back to equation (54),
\begin{align*}
  \opn{P}\left(\bs{\theta}|\text{rest}\right) &\propto \exp\left[-\frac{1}{2}\left( \bs{\theta}'\bs{G}\bs{\theta}+\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}\left(\bs{\zeta}_{i}\left(t_{ij}, t_{i,j-1}\right) - \bs{\chi}_{i}\left(t_{ij},t_{i,j-1}\right)\bs{\theta}\right)'\bs{\Omega}_{\Delta t_{ij}}\left(\bs{\zeta}_{i}\left(t_{ij}, t_{i,j-1}\right) - \bs{\chi}_{i}\left(t_{ij},t_{i,j-1}\right)\bs{\theta}\right)\right)\right]\\
  &\propto \exp\left[-\frac{1}{2}\left\{\bs{\theta}'\left( \bs{G} + \sum_{i=1}^{n}\sum_{j=1}^{m_{i}}\bs{\chi}_{i}'\bs{\Omega}_{\Delta t_{ij}}^{-1}\bs{\chi}_{i}\right)\bs{\theta} -2\bs{\theta}'\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}\bs{\chi}'_{i}\bs{\Omega}_{\Delta t_{ij}}^{-1}\bs{\zeta}_{i}\right\} \right]
\end{align*}
Therefore, $\bs{\theta}|\text{rest} \sim \mathcal{N}\left(\bs{\mu}_{\bs{\theta}}, \bs{\Sigma}_{\bs{\theta}}\right)$ where
\begin{align*}
  \bs{\Sigma}_{\bs{\theta}} &= \left(\bs{G} + \sum_{i=1}^{m}\sum_{j=1}^{m_{i}} \bs{\chi}'_{i}\bs{\Omega}_{\Delta t_{ij}}^{-1}\bs{\chi}_{i}\right)^{-1}\\
  \bs{\mu}_{\bs{\theta}} &= \bs{\Sigma}_{\bs{\theta}}\left[\sum_{i=1}^{n}\sum_{j=1}^{m_{i}}\bs{\chi}_{i}'\bs{\Omega}_{\Delta t_{ij}}^{-1}\bs{\zeta}_{i} \right]
\end{align*}
Some consideration that should be taken in this step is that calculation of $\bs{\Sigma}$ when computing $\bs{\Omega}_{\Delta t_{ij}}$. The dimension of covariates will most likely exceed two which disables the closed form solution to the equation $\bs{A\Sigma} + \bs{\Sigma A}' = \bs{BB}'$. Therefore, the equation must be solved numerically. Equations of the following form are referred to as the \emph{Sylvester equation}. 
\begin{equation}
  \bs{AX} + \bs{XB} = \bs{C}
\end{equation}
given matrices $\bs{A},\bs{B}$ and $\bs{C}$, the problem is to find the possible matrices $\bs{X}$ that satisfy the equation.
Since most computer programming languages call Fortran subroutines in LAPACK or BLAS for matrix operations, either use \texttt{ZTGSYL} from Fortran or equivalent function calls in other languages. \texttt{C++ Armadillo} has the function \texttt{syl} that serves the same purpose.
\end{document}