\documentclass[a4paper, 10pt]{book}
\usepackage{titlesec}
\usepackage{kotex}
\usepackage[margin=0.5in]{geometry} % set the margins to 1in on all sides
\usepackage{graphicx} % to include figures
\usepackage{amsmath} % great math stuff
\usepackage{amsfonts} % for blackboard bold, etc
\usepackage{amsthm} % better theorem environments
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{array}
\usepackage{courier}
\usepackage[usenames, dvipsnames]{color}
\usepackage{titlesec}
\usepackage{empheq}
\usepackage{tikz}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}
\usepackage[english]{babel}

\linespread{1.5}

\titleformat
{\chapter}
[display]
{\bfseries\Large}
{Chapter \ \thechapter}
{0.5ex}
{
  \rule{\textwidth}{1pt}
  \vspace{1ex}
  \centering
}
[
\vspace{-0.5ex}
\rule{\textwidth}{0.3pt}
]

\titleformat{\section}[wrap]
{\normalfont\bfseries}
{\thesection.}{0.1em}{}

\titlespacing{\section}{11pc}{1.5ex plus .1ex minus .2ex}{1pc}

\newcommand\tikzmark[1]{%
  \tikz[remember picture,overlay]\node[inner sep=2pt] (#1) {};}
\newcommand\DrawBox[3][]{%
  \tikz[remember picture,overlay]\draw[#1] ([xshift=-3.5em,yshift=7pt]#2.north west) rectangle (#3.south east);}

\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\newcommand\encircle[1]{%
  \tikz[baseline=(X.base)] 
    \node (X) [draw, shape=circle, inner sep=0] {\strut #1};}
 
% Command "alignedbox{}{}" for a box within an align environment
% Source: http://www.latex-community.org/forum/viewtopic.php?f=46&t=8144
\newlength\dlf  % Define a new measure, dlf
\newcommand\alignedbox[2]{
% Argument #1 = before & if there were no box (lhs)
% Argument #2 = after & if there were no box (rhs)
&  % Alignment sign of the line
{
\settowidth\dlf{$\displaystyle #1$}  
    % The width of \dlf is the width of the lhs, with a displaystyle font
\addtolength\dlf{\fboxsep+\fboxrule}  
    % Add to it the distance to the box, and the width of the line of the box
\hspace{-\dlf}  
    % Move everything dlf units to the left, so that & #1 #2 is aligned under #1 & #2
\boxed{#1 #2}
    % Put a box around lhs and rhs
}
}


\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}

\setcounter{secnumdepth}{4}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\definecolor{myblue}{RGB}{72, 165, 226}
\definecolor{myorange}{RGB}{222, 141, 8}

\setlength{\heavyrulewidth}{1.5pt}
\setlength{\abovetopsep}{4pt}


\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\argmin}{\arg\!\min}
\DeclareMathOperator{\Tr}{Tr}

\newcommand{\bd}[1]{\mathbf{#1}} % for bolding symbols
\newcommand{\RR}{\mathbb{R}} % for Real numbers
\newcommand{\ZZ}{\mathbb{Z}} % for Integers
\newcommand{\col}[1]{\left[\begin{matrix} #1 \end{matrix} \right]}
\newcommand{\comb}[2]{\binom{#1^2 + #2^2}{#1+#2}}
\newcommand{\bs}{\boldsymbol}
\newcommand{\opn}{\operatorname}
\begin{document}
\nocite{*}

\chapter{비모수 통계}
\section{소개}
  본 튜토리얼은 한국어가 가장 익숙하며 통계에 대한 기본적인 지식이 있는 독자층을 위해 작성되었다. 이렇게 문서로 남기는 목적으로 가장 크게는 본인이 공부한 것을 정리하려는 것이며 그 다음으로는 헷갈리는 개념을 정리하여 나중에 똑같은 고민을 하는 사람에게 도움이 되었으면 하는 바람이 있다. Dirichlet process와 관련된 많은 모델들을 그 정의로부터 시작해서 추정방법까지 소개하고자 한다. 따라서 Dirichlet process를 소개하기에 앞서 Dirichlet distribution을 먼저 설명하고 Dirichlet process를 정의하며 그 이후에는 추정방법으로 MCMC와 Variational inference를 설명한다. 그리고 그 방법론을 Dirichlet process를 이용한 다양한 모델들에 적용해 보고자 한다. 여력이 된다면 코드까지 작성하여 Github에 올려 공유하는 것까지 목표로 삼아 보겠다. 안 되면 말고. 가장 중점적으로 내가 Dirichlet process를 공부하면서 겪은 어려움을 해소하는 과정에서 얻은 깨달음을 남기고자 하는 것인데, 대표적으로 명시적인 수학적 도출과정을 생략한 문서가 대부분인 까닭에 꽤나 고생한 경험을 바탕으로 자질구레한 유도 과정을 남기려고 한다. 수학이 싫은 사람은 모든 것을 스킵하고 결과만 보아도 좋을 것 같다.
\section{베이지언 비모수}
  먼저 최근의 패러다임이 모수적 모델링에서 비모수적 모델링으로 넘어가고 있다. 이는 실제로 관측된 데이터셋은 복잡하기 때문에 그것에 맞는 모수적인 모델을 찾기란 때때로 매우 어렵기 때문이다. \underline{비모수적}인 모델링이란 명시적인 함수의 꼴을 정해주지 않고, 데이터가 관측됨에 따라 모형이 지니는 정확도와 복잡도가 그에 맞게 변할 수 있도록 설계하는 것을 말한다. 언뜻 이름만 봐서는 모수가 없을 것 같지만 실제로는 그 반대이다. 모수가 무한 개로 늘어날 수 있도록 설계한다. 베이지언 통계에서 비모수 모델을 세울 때는 대부분 함수 공간에 분포를 할당한다. 모델을 세울 때는 항상 정확도를 높이되 과하게 복잡해지지 않도록 통제할 필요가 있는데 베이지언 통계에서는 적절한 사전 분포를 할당함으로써 이러한 작업을 한다. (빈도주의 통계에서는 regularization을 통해서 과도하게 복잡해지는 것에 패널티를 준다. 궁금한 사람은 ridge regression, Tikhonov regularization, LASSO 등의 키워드로 구글링해보면 많은 자료가 나올 것이다.) 관측된 데이터를 $x_{i}$라 할 때 De Finetti의 정리는 exchangeable한 변수들의 결합분포를 다음과 같이 분해할 수 있다고 증명했다:
  $$
    p\left(x_{1},x_{2},\ldots,x_{N}\right) = \int_{\Theta}p\left(\theta\right)\prod_{i=1}^{N}p\left(x_{i}\middle|\theta\right)\,d\theta.
  $$
  일반적으로 De Finetti의 정리는 모수 공간 $\Theta$가 무한 차원의 확률 측도 공간(probability measure space)일 때만 보장된다. 이 때문에 베이지언 쪽으로 연구하는 사람들은 확률 측도(probability measure)의 분포를 만들어내곤 한다. 그중 Dirichlet process는 우리가 원하는 많은 특성을 지니고 있는 아주 좋은 확률 과정(stochastic process)이다. \par
  쉽게 풀어서 설명하기 위해 베이지언에서 확률을 `자신감'으로 해석한다는 사실을 주지할 필요가 있다. 여기서 앞으로 많은 차이가 나게 되는데, 가장 대표적으로 베이지언에서는 확률을 부여할 때 우리가 `모르는 것'에 확률을 부여해야 한다고 믿는다. 그 미지의 수에 대해 내가 얼마나 `자신 있게' 말할 수 있는지가 바로 내가 부여한 확률값이 되는 것이다. 사전 분포를 주는 것 역시 마찬가지이다. 빈도주의적인 선형회귀 모형 $\bs{y} = \bs{X\beta} + \bs{\epsilon}$에서는 데이터가 랜덤으로 발생한다고 하여 데이터에 확률을 부여한다. 하지만 베이지언에서 확률은 내가 모르는 것에 대해 얼마나 자신 있는지를 나타내기 때문에 내가 모르는 모수 $\bs{\beta}$에 부여해야 한다. 지금까지 데이터인 $\bs{y}$와 내가 모르는 모수 $\bs{\beta}$가 선형 관계를 갖는다고 가정했지만 실제로는 어떠한 함수로 연관돼 있는지 모른다면? ($\bs{y}=f\left(\bs{X}\right) +\bs{\epsilon}$)  다시 말해 어떠한 함수 꼴을 갖는지 모른다면 어떻게 해야 할까? `모르는 것'에 분포를 줄 있다면 함수 꼴에도 분포를 줄 수 있지 않을까? 바로 여기서 가우시안 프로세스가 나오는 것이다. 더 나아가 $\bs{\epsilon}$이 정규 분포를 따르는지 모른다면? 정확히 어떤 분포를 따르는지 모른다면 여기에도 분포를 줄 수 있지 않을까? 이때 우리가 쓸 수 있는 것이 디리클레 프로세스이다. 이렇듯 베이지언에서는 모르는 것에 분포를 주고자 다양한 방법론을 생각해내고 실제로 꽤 괜찮은 성능을 보인다. \par
\section{디리클레 분포}
Dirichlet distribution은 Peter Gustav Lejeune Dirichlet라는 독일 수학자의 이름을 따서 이름붙였다. 당시 유럽의 역사적인 배경 때문에 그는 독일인이었지만 그의 할아버지는 프랑스 제국에 속해있던, 현재 벨기에에 해당하는 뒤렌이란 지방 사람이었기 때문에 그의 성은 사실 프랑스식 이름이다. 그래서 Dirichlet를 어떤 사람은 `디리클레'라고 독일식으로 읽기도 하고 `디리슐레'라고 프랑스식으로 읽기도 한다. 아무렴 상관없다. 본 문서에서는 `디리클레'로 표기할 것이며 괄호 안에 영문명을 병기하겠다. 아무튼 각설하고 디리클레 분포(Dirichlet distribution)은 $D$차원 벡터 $\bs{\pi}$를 모수로 하며, 모수 벡터의 각 원소는 0과 1사이의 값을 가지며 합이 1이 된다. ($0\leq \pi_{i} \leq 1,\; i=1,\ldots , D$ 그리고 $\sum_{i=1}^{D}\pi_{i}=1$.) 그리고 그 밀도함수(density function)는 다음과 같다.
\begin{equation}
  p\left(\bs{\pi}\;|\;\beta_{1},\ldots , \beta_{D}\right) = \frac{\Gamma\left(\sum_{i}\beta_{i}\right)}{\prod_{i}\Gamma\left(\beta_{i}\right)}\prod_{i=1}^{D}\pi_{i}^{\beta_{i}-1},\qquad \beta_{i}\geq 0, \; \forall i
\end{equation}
위의 밀도함수는 모수를 다시 잡아서(reparameterize) 다음과 같이 표시할 수 있다.
\begin{equation}
  p\left(\bs{\pi}\;|\;\alpha g_{01}, \ldots , \alpha g_{0D}\right) =\frac{\Gamma\left(\alpha\right)}{\prod_{i}\Gamma\left(\alpha g_{0i}\right)}\prod_{i=1}^{D}\pi_{i}^{\alpha g_{0i}-1}
\end{equation}
여기서 $\alpha = \sum_{i}\beta_{i}$이고 $g_{0i}=\beta_{i}/\sum_{i}\beta_{i}$. $\bs{\pi}$가 위와 같은 디리클레 분포(Dirichlet distribution)을 따른다는 것은 
\begin{equation}
  \bs{\pi}\sim\opn{Dir}\left(\alpha g_{0}\right)
\end{equation}
라고 표기한다. 이 분포의 평균과 분산은 다음과 같다.
\begin{equation}
  \mathbb{E}\left[\pi_{i}\right] =g_{0i}, \qquad \opn{Var}\left[\pi_{i}\right] = \frac{g_{0i}\left(1-g_{0i}\right)}{\alpha+1}
\end{equation}
베이지언 통계에서 디리클레 분포(Dirichlet distribution)이 중요한 위치를 차지하는 이유는 다항 분포에 대한 켤레사전분포(conjugate prior, 공액사전분포라는 말은 옛날 말이므로 `켤레'라는 말로 이해하기 편하게 바꾸자.)이기 때문이다. 켤레사전분포는 말 그대로 무언가와 켤레인 것이다. 그 대상은 바로 사후분포(posterior)이다. 사전분포와 사후분포가 같은 분포족(family of distributions)에 속해있으면 그 사전분포를 켤레사전분포라고 부른다. 즉, 분포는 변하지 않고 모수만 다르다. 켤레사전분포를 찾는 것이 중요한 이유는 베이즈 정리를 이용해 사후분포를 구하려고 할 때 다음과 같은 계산을 해야 한다.
\begin{equation}
  p\left(\theta|X\right) = \frac{p\left(X|\theta\right)p\left(\theta\right)}{\int p\left(X|\theta\right)p\left(\theta\right)\,d\theta}
\end{equation}
분모에 있는 적분이 존재하기는 하지만 우리가 아는 함수로 표현이 안 될 때가 많다. 이것을 영어로 `mathematically intractable' 혹은 `closed form expression'이 없다고 표현한다. 그리고 이 적분을 피해가기 위해 가장 먼저 생각해낸 것이 켤레사전분포를 이용하는 것이고 최근에 개발된 것이 MCMC(Markov Chain Monte Carlo)와 Variational inference이다. 아무튼 다항분포에 디리클레 사전분포를 부여했을 때 그 사후분포가 어떻게 되는지 보자.
\begin{equation}
  p\left(\bs{\pi}|X=i\right) \propto p\left(X=i|\bs{\pi}\right)p\left(\bs{\pi}\right)
\end{equation}
카테고리가 $i$인 변수 $X$가 관측되었을 때 업데이트된 디리클레 사후분포는 그 카테고리에 해당되는 확률 모수에 1을 더해주는 것이다.
\begin{equation}
  p\left(\bs{\pi}|X=i\right)\propto \pi_{i}^{\left(\alpha g_{0i}+1\right)-1}\prod_{j\neq i}\pi_{j}^{\alpha g_{0j}-1}
\end{equation}
만약 다양한 범주의 변수들이 관측되었다면 똑같이 각 카테고리에 해당되는 관측치의 개수만큼 해당 확률 모수에 더해주면 된다.
\begin{equation}
  p\left(\bs{\pi}|X_{1}=x_{1}, \ldots, X_{N}=x_{N}\right)\propto \prod_{i=1}^{D}\pi_{i}^{\alpha g_{-i}+n_{i}-1}
\end{equation}
여기서 $n_{i}$는 범주 $i$에 해당되는 관측치의 개수이다. 따라서 전체 관측치의 수 $N$은 각 범주에 해당되는 관측치를 다 더한 것이므로 $N=\sum_{i=1}^{D}n_{i}$라는 관계식이 성립한다. 넘나 좋은 것은 아까 말했듯이 켤레사전분포를 갖다 쓰게 되면 사후분포 업데이트가 단순하게 모수 업데이트가 된다는 점이다. 사후분포를 다시 써보자.
\begin{equation}
  \bs{\pi}|X_{1},\ldots ,X_{N}\sim \opn{Dir}\left(\alpha g_{01}+n_{1}, g_{02}+n_{2}, \ldots , g_{0D}+n_{D}\right)
\end{equation}
그리고 각 원소의 평균을 구해보면 다음과 같다.
\begin{equation}
  \mathbb{E}\left[\pi_{i}|X_{1}=x_{1},\ldots , X_{N}=x_{N}\right] = \frac{n_{i}+\alpha g_{0i}}{\alpha + N}
\end{equation}
위의 평태는 원래 나의 믿음을 나타냈던 사전분포와 관측치 사이의 가중평균 형태로 나온다. 후에 디리클레 프로세스(Dirichlet process)로 넘어가면 비슷한 형태를 보게 될 것이므로 이 평균의 형태를 기억해 놓도록 하자.
\section{Pólya Urn Scheme}
폴리아 항아리 이야기라고 번역하겠다. 디리클레 프로세스(Dirichlet process)에는 이야기가 많다. 직관적으로 이해를 돕기 위해 만들어진 것 같다. 하지만 결국 추정의 문제에서는 수학적으로 접근해야 하므로 이해는 하도록 하되 저것만 이해하는 것은 곤란하다. 아무튼 폴리아 항아리 이야기가 무엇이냐 하면, 어떤 항아리에 $D$개의 색깔을 가질 수 있는 공이 $\alpha$개 있다고 하자. 그중 $\alpha g_{01}$개는 첫번째 색깔을 띄고 $\alpha g_{01}$개는 두번째 색깔, 쭉쭉쭉 나간다고 해보자. 어떤 사람이 랜덤으로 공을 뽑고, 그 공을 다시 항아리에 넣고 그것과 똑같은 색깔의 공을 어디선가 가져와서 또 넣는다고 해보자. (다시 말해 원래보다 관측된 색깔의 공이 1개 더 늘어나는 것.) 한 번 이 사람이 뽑고 나서 (조건부) 두번째 공을 뽑았을 때 그 공이 가질 수 있는 색깔의 확률은 다음과 같다.
\begin{equation}
  p\left(X_{2}|X_{1}=x_{1}\right) = \frac{1}{\alpha+1}\delta_{x_{1}} + \frac{\alpha}{\alpha+1}g_{0}
\end{equation}
그리고 이를 계속계속 해 나가면 다음과 같은 관계식이 성립한다.
\begin{equation}
  p\left(X_{N+1}|X_{1}=x_{1},\ldots , X_{N}=x_{N}\right) = \sum_{i=1}^{D}\frac{n_{i}}{\alpha+ N}\delta_{i} + \frac{\alpha}{\alpha + N}g_{0}
\end{equation}
여기서 $\delta_{i}$는 Dirac-delta function으로 공의 색깔이 $i$일 때 1의 값을 지니고 나머지는 0의 값을 갖게 된다. 바로 앞에서 구했던 사후분포와 닮았다. 이것은 (1.12)가 (1.9)에서 구했던 사후분포의 `예측분포(predictive distribution)'이기 때문이다. 예측분포는 앞서 관측했던 $N$개의 관측치로 사후분포를 구하고 나서 $N+1$번째 관측치가 가지는 값의 분포이며 다음과 같이 정의된다.
\begin{equation}
  p\left(X_{N+1}|X_{1}=x_{i},\ldots , X_{N}=x_{N}\right)=\int p\left(X_{N+1}|\bs{\pi}\right)p\left(\bs{\pi}|X_{1}=x_{1},\ldots , X_{N}=x_{N}\right)\,d\bs{\pi}
\end{equation}
여기서 주목할 것은 변수 $X_{1},\ldots , X_{N+1}$사이의 exchangeability(교환가능성)이다. Exchangeability는 iid가정과는 다르다. 서로 dependent하더라도 값을 $x_{1},\ldots , x_{N+1}$을 고정해놓고 어떤 변수가 무슨 값을 갖는지를 바꾸더라도 분포는 변하지 않음을 의미한다. 이게 알고 보면 굉장히 편리한 속성이다. 왜냐하면 나중에 $X_{i}|X_{-i}$를 계산할 때 그냥 $X_{i}$를 맨 마지막에 오는 변수라고 생각하고 앞에서 구한 예측분포를 그대로 적용할 수 있기 때문이다. 즉, $X_{i}$를 제외한 모든 변수들이 앞에 온다고 생각하고 $X_{i}$가 마지막에 온다고 가정해도 아무런 문제가 없는 이유가 바로 exchangeability 때문이다. 다음의 중국집 프로세스(Chinese restaurant process)를 보자.
\section{Chinese restaurant process}
어떤 중국집은 크기가 매우 커서 아무리 많은 손님이 와도 계속 수용할 수 있으며, 새 테이블에 앉겠다고 할 경우 어디선가 새 테이블을 구해다 준다. 손님이 계속계속 들어와 앉는데 이 중국음식점의 손님들은 매우 사교성이 좋아서 $i$번째 손님은 각 테이블에 있는 손님의 수에 비례해서 해당 테이블에 앉을 확률을 갖고, 새 테이블에 앉을 확률은 $\alpha$이다. 다시 말해, 어떤 테이블에 손님이 많으면 그 다음 손님이 그 테이블에 앉을 확률이 높다. 수학적으로 표현하고 이제 관측치(손님)을 $\theta_{i}$로 표시하겠다. 먼저 $N$명의 손님이 앉아있다고 하자. 즉 $\left\{\theta_{1},\ldots , \theta_{N}\right\}$이 관측되었다. $N$명의 손님이 앉아있는 테이블의 수는 많아봐야 $N$개이므로 테이블의 수를 $K$라고 표시하자. 각 테이블에 앉아있는 손님의 수를 $n_{i},\; i=1,\ldots, K$라고 하면, $N+1$번째 손님, $\theta_{N+1}$이 어디에 앉을지 분포를 다음과 같이 구할 수 있다.
\begin{equation}
  \theta_{N+1}|\theta_{N},\ldots , \theta_{1}\sim \sum_{i=1}^{K}\frac{n_{i}}{\alpha + N}\delta_{\theta_{i}} + \frac{\alpha}{\alpha + N}G_{0}
\end{equation}
즉 $G_{0}$는 어떤 분포인데, 마지막에 들어온 관측치 $\theta_{N+1}$는 앞서 관측된 $\theta_{1},\ldots,\theta_{N}$ 중 어느 하나와 같은 값을 가질 수도 있고 $G_{0}$에서 새 값을 추출해서 완전히 새로운 값을 취할 수도 있다. 이런 두 이야기를 염두에 두고 Dirichlet process를 보자.
\section{Dirichlet process}


  확률 과정(stochastic process)들은 무한 차원의 공간을 모델링하는 데 많이 쓰이고, 이를 위해 만들어지기도 한다. 베이지언 비모수 역시 무한 차원의 공간을 모델링하기 위해 확률 과정을 많이 쓰는데, 사실 무한 차원은 이해하기 어렵기 때문에 이 과정(process)들을 정의할 때 유한 개만 가져오면 무엇이 되는지로 정의할 때가 많다. 대표적인 것이 Gaussian process이다. Gaussian process는 주로 비선형 회귀 모형에서 함수의 꼴을 모르기 때문에 무한 차원의 함수 공간을 모델링하기 위해 쓰이는데 그 정의는 다음과 같다. 어떤 함수 $f\; : \; \mathcal{X}\mapsto \mathbb{R}$가 Gaussian process를 따른다 함은 $N$개의 $x_{i}\in \mathcal{X}$를 임의적으로 가져왔을 때 이들의 결합 분포 $p\left(f\left(x_{1}\right),\ldots ,f\left(x_{N}\right)\right)$가 정규분포를 따른다는 말과 동치이다. 이렇게 정의하게 되면 Gaussian process는 적절한 평균함수(mean function)과 공분산 함수(covariance function)로 정의될 수 있고, 이를 사용하였을 때 추정을 위한 계산이 쉬워진다. \par
  Gaussian process가 알지 못하는 함수를 모델링할 때 쓰인다면 Dirichlet process는 알지 못하는 확률 측도(probability measure) 혹은 적분했을 때 값이 1이 되는 0이 아닌 함수들의 분포를 모델링할 때 쓰인다. 모수 공간을 $\Theta$라고 했을 때 Dirichlet process는 $\Theta$에 대한 기저 측도(base measure) $H$와 집적 모수(concentration parameter) $\alpha$로 정의할 수 있다. 가우시안 프로세스가 임의로 뽑은 $N$개의 유한한 데이터가 어떤 분포를 가지는지로 정의되었던 것처럼, 디리클레 프로세스 역시 모수 공간 $\Theta$의 분할(partition)에 대해서 기저 측도가 어떤 분포를 만들어내는지로 정의할 수 있다.
  \begin{thm}
    $H$가 측정가능한 공간(measurable space) $\Theta$에 대한 확률 분포이고 $\alpha$가 어떤 양의 상수일 때, 모수 공간 $\Theta$의 유한한 분할 $\left(A_{1},\ldots , A_{N}\right)$에 대해 다음이 성립한다.
    $$
      \bigcup_{i=1}^{N}A_{i} = \Theta \hspace{35pt} A_{i}\cap A_{j} = \emptyset,\; i \neq j
    $$
    우리는 모수 공간 $\Theta$에 대한 임의의 확률 분포 $G$가 디리클레 프로세스를 따른다고 함은 위와 같은 성질을 만족하는 모든 유한한 분할에 대한 측도가 다음과 같이 디리클레 분포를 따른다는 말과 동치이다.
    $$
      \left(G\left(A_{1}\right),\ldots ,G\left(A_{N}\right)\right) \;\sim\; \opn{Dir}\left(\alpha H\left(A_{1}\right),\ldots , \alpha H\left(A_{N}\right)\right)
    $$
    어떠한 기저측도 $H$와 집적 모수 $\alpha$라도 위와 같은 성질은 만족하는 유일한 확률과정이 존재하는데 이를 우리는 $\mathcal{DP}\left(\alpha, H\right)$라고 표기한다.
  \end{thm}
  Kolmogorov's consistency condition에 따르면 어떠한 유한한 부분집합에도 동일한 성질을 유도해 낼 수 있으면, 다시 말해 모수공간을 어떻게 분할해도 그게 Dirichlet distribution을 따르면 그런 확률과정(stochastic process)가 존재한다. 이러한 확률 과정이 존재한다는 사실을 퍼거슨(Ferguson)이 콜모고로프(Kolmogorov)의 일치성 조건(consistency condition)을 이용하여 밝혀냈는데, 훗날 세스라만(Sethuraman)이 더 간단한 정의로 쉽게 똑같은 것을 증명해내는 데 성공한다.
\section{디리클레 프로세스의 성질}
  앞서 설명한 수학적인 디리클레 프로세스의 정의는 측도 이론(measure theory)를 이해하고 있으면 더 편하게 다가오지만 그렇지 않은 사람에게는 좀 어렵다. 어느 정도의 측도 이론은 공부할 필요가 있는 듯싶다. 하지만 최대한 쉽고 직관적으로 설명해 보려고 노력하겠다. 아무튼 어떠한 확률 분포에서 임의의 확률 변수를 추출하면 그 확률 분포가 지니는 정의역에서 하나의 값이 튀어나온다. 이를 샘플링한다고 말하는데, 예를 들어 포아송 분포에서 샘플링하게 되면 음이 아닌 정수들이 튀어나오게 되고, 각 값이 튀어나오는 빈도는 포아송 분포가 지니는 확률값에 의존한다. 또 정규분포에서 샘플링하면 실수 수직선 상에서 값들이 나오게 되는데 그 빈도 역시 평균에 가까운 값은 많이 나오는 형태로 나오게 된다. 즉 밀도함수의 함수값과 비례해서 그 정의역 값이 추출되는 것이다. 디리클레 분포는 일명 `분포의 분포'로 이로부터 샘플링하게 되면 하나의 분포가 나오는 것이다. 어떤 모수 공간의 부분집합 $A \subset \Theta$에 대해 디리클레 분포에서 추출된 샘플이 할당하는 값의 평균은 다음과 같이 쓸 수 있다.
  $$
  \mathbb{E}\left[G\left(A\right)\right]=H\left(A\right) \hspace{35pt} G\sim \mathcal{DP}\left(\alpha, H\right).
  $$
  이 때문에 기저 측도라 부르고 있는 $H$는 디리클레 분포의 평균이 된다. 이와 달리 집적 모수 $\alpha$는 산포도(퍼진 정도)를 결정하게 되는데 디리클레 분포의 정밀도(precision)과 비슷하다고 생각하면 된다. \par
  이제 사후 분포에 대해 이야기해보자. 앞서 말했듯이 디리클레 프로세스에서 샘플을 추출하게 되면 그것은 임의의 확률 분포가 된다. 그럼 일반적으로 했듯이 이 확률 분포에서 또다시 확률 변수를 샘플링할 수 있다. 다음과 같은 구조이다.
  \begin{align*}
    G &\sim \mathcal{DP}\left(\alpha, H\right)\\
    \theta\;|\; G &\sim G
  \end{align*}
  잠깐 일반적인 정규분포 표본의 문제로 돌아가보자. 문제가 다음과 같을 때 모수의 사후 분포를 계산하는 것을 다시 한 번 생각해보자.
  \begin{align*}
    &\mu \;\sim\; \mathcal{N}\left(\mu_{0},\sigma_{0}^{2}\right)\\
    &X_{1},X_{2},\ldots , X_{N}\;|\;\mu \sim \mathcal{N}\left(\mu,\sigma^{2}\right)
  \end{align*}
  이럴 때 우리는 평균 $\mu$의 사후 분포 $\mu\;|\;X_{1},\ldots X_{N}$를 구할 수 있었다. 동일하게 확률 분포 $G$의 사후분포를 구할 수 있을 것이란 생각은 굉장히 자연스럽다. 이는 디리클레 분포의 켤레성(conjugacy) 덕에 쉽게 구할 수 있다. 예를 들어, $\theta$가 모수 공간을 분할한 것 중 하나에 속한다면 디리클레 분포의 모수 중 똑같은 부분에 하나를 더해주면 된다.
  $$
  \left(G\left(A_{1}\right),\ldots ,G\left(A_{N}\right)\;|\;\theta\in A_{n} \right) \;\sim\; \opn{Dir}\left(\alpha H\left(A_{1}\right),\ldots , \alpha H\left(A_{n}\right), \ldots , \alpha H\left(A_{N}\right)\right),\quad n\in\left\{1,\ldots , N\right\}
  $$
  이로부터 자연스럽게 $G$의 사후분포를 유도해낼 수 있는데 그것은 다음과 같이 정리할 수 있다.
  \begin{thm}
    $G \sim \mathcal{DP}\left(\alpha, H\right)$가 임의의 확률 분포라고 하자. 그리고 이로부터 $N$개의 표본 $\theta_{i} \;\sim\; G$을 관측하였을 때 사후 분포 역시 업데이트된 디리클레 프로세스가 된다.
    $$
      G\;|\;\theta_{1},\ldots ,\theta_{N},\alpha,H \;\sim\; \mathcal{DP}\left(\alpha+ N, \frac{1}{\alpha + N}\left(\alpha H+\sum_{i=1}^{N}\delta_{\theta_{i}}\right)\right)
    $$
  \end{thm}
\section{Stick-Breaking process}
  앞장의 모든 설명은 수학적으로는 증명이 되고 유용하다는 것이 입증되기는 했지만 실제로 디리클레 분포로부터 어떻게 샘플을 추출할 수 있는지 그 매커니즘은 제공하지 않는다. 세스라만(Sethuraman---사실 어떻게 음역해야 할지 참 곤란하다---)은 stick-breaking construction이라는 이름으로 이를 해결하고 더 나아가 예측 분포가 어떻게 생기는지에 대한 프로세스인 Chinese restaurant process를 고안하는 토대가 된다. 디리클레 프로세스에서 추출된 분포는 모두 이산형이다. 이를 염두해 두고 앞에서 구한 사후 분포를 다시 생각해보자. 이에 따르면 똑같이 모수 공간의 어떤 부분집합 $A$에 대해 사후 분포는 다음과 같은 평균을 갖는다.
  \begin{equation}
    \mathbb{E}\left[G\left(A\right)\;|\;\theta_{1},\ldots , \theta_{N},\alpha, H\right] = \frac{1}{\alpha+H}\left(\alpha H\left(A\right)+\sum_{i=1}^{N}\delta_{\theta_{i}\left(A\right)}\right)
  \end{equation}
  여기에서 $N\to\infty$하게 되면 평균은 $\sum_{k=1}^{\infty}\pi_{k}\delta_{\theta_{k}}\left(A\right)$가 된다. 여기서 $\pi_{k}$는 무한 개의 관측치 $\theta_{1},\theta_{2},\ldots$에서 $\theta_{k}$의 상대 빈도를 나타낸다. 그러니까 무한 개에서 특정한 관측치의 개수를 세서 전체에서의 상대 빈도를 계산한다는 것은 그 비율이 어떤 수로 수렴한다는 것을 의미한다. 이는 디리클레 프로세스에서 표본이 되는 확률 분포를 추출하는 메커니즘을 제공한다.
  \begin{thm}
    $G\;\sim\; \mathcal{DP}\left(\alpha, H\right)$라고 할 때, 다음과 같이 재현될 수 있다.
    \begin{align*}
      &\beta_{k}\;\sim\;\opn{Beta}\left(1,\alpha\right)\hspace{100pt} k=1,2,\ldots\\
      &\pi_{k} = \beta_{k}\prod_{\ell=1}^{k-1}\left(1-\beta_{\ell}\right) = \beta_{k}\left(1-\sum_{\ell=1}^{k-1}\pi_{\ell}\right)\\
      &G\;=\;\sum_{k=1}^{\infty}\pi_{k}\delta_{\theta_{k}}\hspace{111pt} \theta_{k}\;\sim\; H
    \end{align*}
  \end{thm}
  
  베타 분포의 샘플과 기저 측도에서 추출한 샘플을 적당히 섞어주면 그것의 무한합은 디리클레 프로세스를 따르게 된다. 이를 통해 실질적으로 컴퓨터로 디리클레 프로세스를 구현할 수 있게 되었다. 이름이 의미하는 바가 큰데 이것을 stick-breaking process라고 명명한 데에는 꽤나 직관적인 해석이 숨어있다. 베타 분포에서 처음 $\beta_{1}$을 추출했을 때는 0과 1 사이의 어떤 사이를 뽑음으로써 길이가 1인 막대기를 2개로 부러뜨린 것과 같다. 그리고 $\pi_{k}$를 정의한 것을 자세히 보면 $\beta_{1}$을 뽑고 남은 부분에 대해서---$\left(1-\beta_{1}\right)$---다시 $\beta_{2}$의 비율만큼 부러뜨리고, 또 남은 것을 $\beta_{3}$의 비율로 부러뜨리고 이 과정을 반복한다. 마지막에 $\pi_{k}\delta_{\theta_{k}}$의 의미는 실수 축에서 $\theta_{k}$자리에 $\pi_{k}$만큼 확률값을 주자는 말이다. 이를 흔히 `확률질량'이라고 부른다.
\chapter{Inference}
\section{Introduction}
  이전에도 말했듯이 베이지언은 가지고 있는 데이터를 바탕으로 추정하고자 하는 모수의 조건부 분포를 구하는 것이 추정의 핵심이다. 이를 위해 필요한 장치(machinery)로서 사전분포(prior distribution)가 필요하고, 매커니즘으로서 베이즈 정리가 필요하다. 그 과정에서 적분을 해야 하고 많은 경우 적분이 불가능하다. (이것은 적분값이 무한대이거나 적분이 아예 안되는 함수라는 뜻이 아니라, 정적분은 존재하지만 원시함수 혹은 부정적분이 초등함수(elementary function)의 결합으로 표현될 수 없다는 뜻이다. 이에 대해 더 궁금한 사람은 Liouville's theorem (1835)을 구글링해보기 바란다.) 대표적으로 적분값은 존재하지만 부정적분이 없는 함수가 가우스 함수 $f\left(x\right)= e^{-x^{2}}$이다. 아무튼 베이지언 추정에서 이 적분을 비껴가고자 갖은 노력을 다한다.\par
  그리고 마침내 세기의 알고리즘이 탄생하게 되는데 이름하여 MCMC(Markov Chain Monte Carlo)이다. 이 알고리즘은 원자폭탄 투하를 위해 중성자들이 원자들과 충돌할 확률을 추정하기 위해 미국에서 진행했던 ``Manhattan project''에서 개발되었다\cite{manhattanproject}. 당시 유명한 물리학자였던 Robert Oppenheimer는 젊은 물리학자였던 Nick Metropolis를 영입했고, Metropolis는 John von Neumann(그 폰노이만 맞다), Stanislaw Ulam, Robert Richtmyer와 함께 컴퓨터를 이용한 새로운 알고리즘을 개발한다. 그래서 MCMC 중에 Metropolis-Hastings 알고리즘이 있다.\par
  MCMC 중에 가장 대표적인 알고리즘이 Gibbs sampling이란 건데 이는 Metropolis-Hastings 알고리즘의 특별한 형태이다. 이것 역시 물리학자였던 Josiah Willard Gibbs라는 사람의 이름을 땄고, Gibbs sampling은 Gibbs가 죽고 80년이 흐른 뒤 Geman 형제에 의해 재발견되어 논문에 게재된다. 아무튼 역사 이야기는 재미로 읽는 것으로 하고 그 원리에 대해서 설명해보자. \par
  이번 장은 Markov chain에 대해서 기본적인 지식이 있다고 가정하고 시작한다. Markov chain을 처음 듣는 사람은 다시 구글의 힘을 빌려 공부하는 것이 좋겠다. 먼저 MCMC의 기본적인 아이디어는 사후분포(posterior)를 명시적으로 구할 수 없으니, 어떤 Markov chain을 잘 만들어서 그 stationary distribution(혹은 invariant distribution)이 posterior가 되어 초기값을 주고 chain을 끝없이 돌리면 우리가 원하는 posterior로 수렴하도록 하는 것이다. 그중에서도 Gibbs sampling은 제일 단순하면서도 강력한 형태인데, 각 변수의 posterior는 못 구해도 full conditional은 구할 수 있을 때 쓸 수 있다. 여기서 full conditional이라 함은 자기 자신을 제외한 모든 변수에 conditioning된 형태를 말한다. 따라서 $X_{1}, X_{2}, X_{3}$ 세 개 변수가 있다면 다음과 같이 Gibbs sampler를 짤 수 있다.
  \begin{enumerate}
  \item $X_{1}^{\left(t+1\right)}\sim p\left(X_{1}|X_{2}^{t}, X_{3}^{t}\right)$
  \item $X_{2}^{\left(t+1\right)}\sim p\left(X_{2}|X_{1}^{t+1}, X_{3}^{t}\right)$
  \item $X_{3}^{\left(t+1\right)}\sim p\left(X_{3}|X_{1}^{t+1}, X_{2}^{t+1}\right)$
  \end{enumerate}
  Full conditional은 간편하게 $p\left(X_{i}|X_{-i}\right)$로 표기한다. 이런 식으로 끝없이 chain을 돌리면 posterior로 수렴하게 되는데, 우리가 짠 것은 sampler이기 때문에 분포의 꼴은 나오지 않고 posterior에서 추출한 샘플이 모이게 된다. 표본을 가지고 강대수의 법칙을 따라 posterior mean을 구하는 등 추정에 이용할 수 있다.

  \chapter{Dirichlet process mixture}
  \section{Model specification}
  이번 장에서는 최근에 각광받고 있는 clustering 기법 중 generative한 모델인 Dirichlet process mixture에 대해서 알아보도록 하자. 지금까지 관측치들이 하나의 모수적인 분포에서 전부 생성되었다고 모델링하였으나 이것은 매우 강한 제약 조건이다. 왜냐하면 실제 데이터를 봤을 때 우리는 그 데이터가 어떤 분포에서 생성되었는지 모르기 때문이다. 분포를 모를 때 Dirichlet process를 사용할 수 있다고 들었으니 데이터 $\left\{x_{1},\ldots x_{N}\right\}$가 $G$라는 분포에서 생겨났고 그 $G$는 또 $\mathcal{DP}\left(\alpha, H\right)$에서 추출된 분포라고 가정을 할 수 있겠다. 하지만 이 또한 문제인 것이 Dirichlet process에서 생성된 분포는 almost surely discrete, 그러니까 이산형 분포가 되고 관측치가 완전히 이산형 분포에서 나왔다고 가정하는 것 역시 너무 강한 제약조건이며, 이런 가정을 할 경우 사후분포가 inconsistent하다는 논문이 있다. 따라서 위와 같은 문제를 피해가기 위해 연속형 분포와 Dirichlet process를 `convolve'한다고 하는데 우리말로 하면 합성한다고 보면 되겠다. 모델을 설명하자면 아래와 같다.
  \begin{align*}
    \bs{\pi} &\sim \opn{GEM}\left(\alpha\right)\\
    z_{i} &\sim \opn{Discrete}\left(\bs{\pi}\right)\\
    \theta_{k} &\sim H\left(\lambda\right)\\
    \bs{x}_{i} &\sim F\left(\theta_{z_{i}}\right)
  \end{align*}
  여기서 $H$는 $G \sim \mathcal{DP}\left(\alpha, H\right)$의 기저분포로서의 $H$이다. Stick-breaking construction을 쓴 것이다. 그러니까 각 데이터 관측치 $\bs{x}_{i}$들이 어떤 분포의 패밀리($F$)에서 생성되는데 그 모수는 다를 수 있다고 가정하고, 서로 다른 $F$가 이론상 무한 개로 늘어날 수 있지만 실질적으로는 데이터의 개수인 $N$개까지 늘어날 수 있도록 하는 것이다. Finite mixture model에서는 서로 다른 $F$의 개수(=클러스터의 개수)를 $K$개라고 정하고 시작했지만 여기서는 수집된 데이터에 맞게 알아서 스스로 적합되도록 모델링한 것이다.
  \section{Gibbs sampler for DPM}
  Dirichlet process mixture 모델에서 Gibbs sampler는 어떻게 계산하는지 보자. 위의 모델에서 우리가 추정해야 하는 모수는 
  \begin{equation}
    \mathcal{V}=\left\{z_{1},\ldots, z_{N}, \theta_{1},\ldots, \right\}
  \end{equation}
  인데 일반적인 Gibbs sampler보다 collapsed Gibbs sampler가 더 수렴속도가 빠르므로 collapsed Gibbs sampler를 계산해보도록 하자. Collapsed Gibbs sampler는 \emph{Rao-Blackwellization}을 통해 조건부로 걸려있는 변수를 marginalize out한 Gibbs sampler를 말한다. 예를 들어 일반적인 Gibbs sampler가 다음처럼 생겼다면
  \begin{enumerate}
    \item $\xi^{\left(t\right)} \sim p\left(\xi|\mu^{\left(t-1\right)}, X\right)$
    \item $\mu^{\left(t\right)} \sim p\left(\mu|\xi^{\left(t\right)},X\right)$
  \end{enumerate}
  
  collapsed Gibbs sampler는 아래와 같이 생겼다.
  \begin{enumerate}
    \item $\xi^{\left(t\right)}\sim p\left(\xi|\mu^{\left(t-1\right)},X\right)$
    \item $\mu^{\left(t\right)}\sim p\left(\mu|X\right)$
  \end{enumerate}
  
  Collapsed Gibbs sampler는 이 경우 $\mu$의 주변 사후분포(marginal posterior distribution)으로부터 곧바로 샘플링해낸다. 이렇게 하는 이유는 수렴 속도가 확실히 빨라지기 때문이지만 대신에 $\xi$와 $\mu$ 사이의 종속관계는 깨진다. 아무튼 이 문제는 나중에 다루도록 하고, DPM에서의 collapsed Gibbs sampler를 구하기 위해 $\theta$를 적분해서 버리도록 하자(marginalize out). 그럼 결국 남는 것은 $z_{1}, z_{2}, \ldots , z_{N}$들이다. 따라서 다음의 확률을 구해야 한다.
  \begin{equation}
    p\left(z_{i}=k|z_{-i}, \bs{x}, \alpha, \lambda\right)
  \end{equation}
  위의 확률은 다음과 같이 분해될 수 있다.
  \begin{equation}
    p\left(z_{i}=k|z_{-i}, \bs{x}, \alpha, \lambda\right) \propto p\left(z_{i}=k|z_{-i}, \alpha\right)p\left(\bs{x}_{i}|\bs{x}_{-i}, z_{i}=k, z_{-i}, \lambda\right)
  \end{equation}
  왜 이렇게 분해될 수 있는지는 모델을 세울 때 이미 나온다. $\bs{x}_{i}$와 $z_{i}$들은 서로 독립일 수가 없다. 세팅 상 $z_{i}$가 $\bs{x}_{i}$가 어느 클러스터에 속하는지를 나타내는 indicator variable이라서 서로 독립이 아니다. 반면 $z_{i}$와 $\theta_{k}$들은 서로 독립이다. 그렇기 때문에 $\theta_{k}$의 hyperparameter인 $\lambda$는 $z_{i}$와 전혀 무관하다. 이런 모수들의 관계는 Graphical model 도표를 보면 더 직관적으로 이해할 수 있다. 이를 위해서는 다른 참조 문헌을 참조하는 것이 좋겠다.
% \section{폴리아 항아리\\(Pólya Urns)}
%   디리클레 프로세스의 표본은 이산형 분포가 되기 때문에 필연적으로 이 분포의 샘플은 같은 값을 가지는 관측치가 생기게 된다. 따라서 $N$개의 관측치 $\theta_{1},\ldots , \theta_{N}$를 관측했다 하더라도 이들이 지니는 값은 모두 다르면 $N$개, 겹치는 게 있으면 $N$보다 작아지게 된다. 따라서 이 서로 다른 값을 $\left\{\theta_{k}'\right\}_{k=1}^{K},\; K\leq N$라고 쓰면 (1.1)을 아래와 같이 다시 쓸 수 있게 된다.
%   $$
%     \mathbb{E}\left[G\left(A\right)\;|\;\theta_{1},\ldots , \theta_{N},\alpha, H\right] = \frac{1}{\alpha+N}\left(\alpha H\left(A\right)+\sum_{k=1}^{K}N_{k}\delta_{\theta_{k}'}\left(A\right)\right)
%   $$
%   여기서 $N_{k}$는 $\theta_{k}'$의 값을 지니는 관측치의 개수이다.
%   \begin{thm}
%     $G\;\sim\;\mathcal{DP}\left(\alpha, H\right)$에서 기저 측도 $H$가 밀도함수 $h\left(\theta\right)$를 가지고, $N$개의 관측치 $\theta_{i}\;\sim\; G$가 $K$개의 서로 다른 값 $\left\{\theta_{k}'\right\}_{k=1}^{K}$를 가진다고 하자. 다음 관측치의 예측 분포는 다음과 같다.
%     \begin{equation}
%       p\left(\theta_{N+1}=\theta' \;|\; \theta_{1},\ldots , \theta_{N},\alpha, H\right)=\frac{1}{\alpha+N}\left(\alpha h\left(\theta'\right)+\sum_{k=1}^{K}N_{k}\delta\left(\theta',\theta'_{k}\right)\right)
%     \end{equation}
%     여기서 $N_{k}$ $\theta_{1},\ldots , \theta_{N}$에서 값 $\theta_{k}'$을 지니는 것들의 개수이다.
%   \end{thm}
  
%   즉 (1.2)를 인간의 말로 풀면, 예측 분포는 이미 관측된 값들에서 나올 확률과 아예 새로운 값이 관측될 확률로 구성되어 있다. 그리고 이미 관측된 값에서 뽑힐 확률은 각 값이 이전에 얼마나 빈번하게 관측되었는지에 비례해서 더 많이 관측된 값이 이후에도 관측될 확률이 높아지게 된다. 이것을 디리클레 분포의 `빈익빈 부익부(The rich get richer)' 성질이라고 한다. 이후에 설명할 믹스쳐 모델(Mixture model)을 만들 때 바로 이 성질이 중요한 역할을 하게 된다. \par
%   이러한 예측모형은 폴리아 항아리 모델의 일반화된 버전이라고 볼 수 있다. 하나의 항아리에는 그전까지의 관측치의 개수만큼 공이 있는데, 서로 다른 값은 서로 다른 색깔의 공으로 표시한다. 그러니까 서로 다른 색깔의 공이 그 값을 지니는 관측치 개수만큼 있다고 생각하면 된다. 그리고 마지막으로 $\frac{\alpha}{\alpha+N}$의 확률로 뽑히는 공이 하나 더 있는데 이 공은 뽑히는 순간 색깔이 결정되고, 그 색깔은 여태 보지 못한 색일 수 있다. 새로운 값을 지니는 관측치가 저 확률로 뽑히게 된다는 뜻이다. 이 이야기를 이용하면 명시적으로 $G\;\sim\;\mathcal{DP}\left(\alpha, H\right)$를 만들지 않고도 관측치들을 샘플링할 수 있다는 장점이 있다.
%   \section{Chinese Restaurant Process}
%     디리클레 프로세스는 관측치 $\theta_{i}$에 값 $\theta_{k}'$를 부여함으로써 클러스터링 효과를 내게 된다. 왜냐하면 이산형 분포에서 샘플링하면 필연적으로 겹치는 값을 갖는 샘플이 존재하기 때문이다. 같은 값을 갖는 샘플을 `같은 클러스터에 속한다'고 생각하면 이게 바로 클러스터링이 된다. 만약 $K$개의 서로 다른 값을 지니는 값 $\left\{\theta_{k}'\right\}_{k=1}^{K}$가 있다면, $i$번째 관측치 $\theta_{i}$가 $K$개 군집 중 어디에 속하는지를 $z_{i}\in\left\{1,\ldots , K\right\}$로 나타낸다면 $\theta_{i}=\theta_{z_{i}}'$로 나타낼 수 있다. 이것을 이용해서 (2.1)을 다시 써보면
%     \begin{equation}
%     p\left(z_{N+1}=z\;|\;z_{1}, \ldots , z_{N},\alpha\right) = \frac{1}{\alpha+N}\left(\sum_{k=1}^{K}N_{k}\delta\left(z,k\right)+\alpha\delta\left(z,k'\right)\right)
%     \end{equation}
%     이 되고 여기서 $k'$는 이전에 관측되지 않은 완전히 새로운 값을 의미한다. 다시 말해 새로운 클러스터가 생긴 것이다. 이것을 스토리텔링 형태로 바꾼 것이 Chinese restaurant process인데 말하자면 이렇다. 손님을 끝도 없이 받을 수 있는 중국 음식점이 있다고 해보자. 손님들은 들어와서 테이블을 잡아서 앉을 텐데 여기서 테이블은 클러스터를 의미한다. 이 음식점의 손님들은 매우 사교적이어서 사람들이 많은 테이블에 가서 앉는 경향이 있다. 하지만 때때로 어떤 손님은 아무도 없는 테이블에 가서 앉는 경우도 있긴 하다. 여기서 주의해야 할 점은 아무도 앉지 않은 테이블은 구분 없이 그냥 다 똑같고, 손님이 새 테이블을 찾아 앉을 때는 아무 데나 가서 앉는다. 디리클레 프로세스의 클러스터링 효과를 아주 재미있게 표현한 이야기이다. 그리고 중요한 것은 이러한 예측 분포의 특징을 이용하면 몬테칼로 방법을 이용하여 추정하려고 할 때 명시적인 형태(explicit form)의 수식으로 표현이 가능해진다는 장점이 있다.
%   \chapter{Mixture Models}
%   \section{Finite Mixture Models(FMM)}
%     Dirichlet process mixtures로 넘어가기 전에 유한한 버전인 finite mixture models에 대해 먼저 설명하려고 한다. 다시 수리통계에서 많이 보았던 다음의 가정을 보자. 편의상 정규분포를 쓰기로 한다.
%     $$
%     X_{1},X_{2},\ldots , X_{n}\;|\;\mu\;\sim\; \mathcal{N}\left(\mu, \sigma^{2}\right)
%     $$
%     모든 관측치들이 하나의 공통된 정규분포에서 생성된다고 모델링한 것이다. 그런데 단 하나의 정규분포에서 모든 관측치들을 생성해낸다고 가정하는 것은 너무 큰 제약조건이다. 그래서 생각해낸 것이 여러 개의 정규분포에서 데이터들을 만들어내고 있다고 가정하고 거기서부터 추정을 하는 것이다. 모집단/표본으로 생각을 한다면 모집단이 하나가 아니라고 가정하는 것과 같다. 여러 개의 모집단에서 표본이 섞여서 관찰된다고 생각하면 쉽다. 처음부터 무한개의 모집단을 가정하면 골치 아프니까 먼저 $K$개의 정규분포가 있다고 생각해보자. (보통 정규분포로 하지만, 지수족(exponential family)의 믹스쳐로 확장해도 무방하다.) 여기서 분산은 알고 있다고 가정한다. 그러면 관측치의 가능도함수(likelihood function)은 다음과 같다.
%     \begin{equation}
%     p\left(x\;|\;\pi,\mu_{1},\ldots , \mu_{K}\right) = \sum_{k=1}^{K}\pi_{k}\mathcal{N}\left(x\;|\;\mu_{k},\sigma^{2}\right)
%     \end{equation}
%     여기서 $\pi_{k}$는 $k$번째 정규분포가 차지하는 비중을 의미한다. 그러니까 표본 중에서 $k$번째 정규분포에서 나온 애들의 비중, 혹은 하나의 관측치가 관측될 때 그것이 $k$번째 정규분포에서 나왔을 확률이다. $\mathcal{N}\left(x\;|\;\mu,\sigma^{2}\right)=f\left(x\;|\;\mu\right)$로 표기할 때, 이는 분포함수 $F\left(\mu\right)$를 가진다. 이것은 다음과 동치이다. 각각의 관측치 $x_{i}$는 $K$개 클러스터 중 하나에서 생성되는데, 각 클러스터가 뽑히는 사건은 multinomial(혹은 Discrete라고 표기하기도 함)을 따른다.
%     \begin{enumerate}
%       \item $z_{i}\;\sim\;\opn{Discrete}\left(\bs{\pi}\right), \qquad$ (클러스터를 뽑는다.)
%       \item $x_{i}\;\sim\;F\left(\mu_{z_{i}}\right), \qquad$ (해당되는 클러스터에서 관측치를 생성한다.)
%     \end{enumerate}
    
%     이때 $z_{i}\in\left\{1,\ldots,K\right\}$이며 $x_{i}$이 엮여있는 클러스터를 나타낸다. 믹스쳐 모델은 관측된 데이터를 가까운 것끼리 하나의 군집으로 묶어 클러스터링하는 이른바 `비지도 학습(unsupervised learning)'에 자주 쓰인다. \par
%   \section{Gibbs Sampler for FMM}
%     이런 모델은 가정일 뿐이고, 실제로 관측되는 데이터는 어떠한 비율로 각각의 클러스터에서 생성되는지 알 턱이 없다. 그렇기 때문에 추정을 해야 하는데, 우리가 알고 싶은 것은 각 클러스터의 모수, $\theta_{k},$와 각 클러스터의 비중, 즉 $\bs{\pi}=\left(\pi_{1},\ldots , \pi_{K}\right)$이다. 모르는 것에 사전 분포를 할당해야 한다고 했으니, $\mu_{k}$에는 켤레사전분포(conjugate prior) $H\left(\lambda\right)$를 부여하고 $\bs{\pi}$역시 Discrete분포에 켤레인 디리클레 분포(Dirichlet distribution)을 부여하도록 하자. 그리고 각각의 관측치들이 어느 클로스터에 해당하는지를 indicator variable인 $z_{i}$로 표시한다고 하면, 이는 Discrete분포를 따르게 될 것이다.
%     \begin{align*}
%       \bs{\pi}\;&\sim\; \opn{Dir}\left(\alpha/K,\ldots ,\alpha/K\right)\\
%       \bs{z}\;|\;\bs{\pi} \;&\sim\; \opn{Discrete}\left(\bs{\pi}\right)\\
%       \theta \;&\sim\; H\left(\lambda\right)\\
%       x_{i}\;|\;\theta_{z_{i}}\;&\sim\; F\left(\theta_{z_{i}}\right)
%     \end{align*}
%     깁스 샘플러를 구하려면 각 모수의 full conditional을 구하면 된다. 그러니까 관측치와 모든 모수의 결합분포, 즉 가능도함수에 사전분포를 모두 곱한 것에서 각 모수가 들어있는 부분만 떼서 곱한 것이 full conditional의 정규화되지 않은 부분이다. 따라서 $i$번째 관측치가 들어있는 클러스터가 $k$번째일 확률의 full conditional은 구해보면
%     \begin{equation}
%     p\left(z_{i}=k\;|z_{\setminus i},\bs{x},\bs{\pi},\theta_{1},\ldots ,\theta_{K}\right) \propto \pi_{k}f\left(x_{i}\;|\; \theta_{k}\right)
%     \end{equation}
%     가 된다. 알고리즘을 구해보면 아래와 같다.
%     \begin{algorithm}
%       \caption{Gibbs sampler for FMM}
%       \begin{algorithmic}[1]
%       \Procedure{GSFMM}{$\bs{x},\text{sampleNumber}, \text{burnIn}, K, \lambda,\alpha$}\label{GSFMM}
%         \State $z_{i}^{\left(0\right)}, \bs{\pi}^{\left(0\right)}, \theta_{k}^{\left(0\right)}$ 초기화
%         \For{$v=1:\text{sampleNumber}$}
%           \For{$w=1:\text{burnIn}$}
%             \State $z_{i}^{\left(t\right)}\;\sim\;\dfrac{1}{Z_{i}}\sum_{k=1}^{K}\pi_{k}^{\left(t-1\right)}f\left(x_{i}\;\middle|\;\theta_{k}^{\left(t-1\right)}\right)\delta\left(z_{i},k\right), \qquad Z_{i}= \sum_{k=1}^{K}\pi_{k}^{\left(t-1\right)}f\left(x_{i}\;\middle|\;\theta_{k}^{\left(t-1\right)}\right)$
%             \State $\bs{\pi}^{\left(t\right)} \;\sim\;\opn{Dir}\left(N_{1}+\alpha/K, \ldots , N_{K}+\alpha/K\right),\qquad N_{k} = \sum_{i=1}^{N}\delta\left(z_{i}^{\left(t\right)},k\right)$
%             \State $\theta_{k}^{\left(t\right)}\;\sim\; p\left(\theta_{k}\;\middle|\;\left\{x_{i}\;|\;z_{i}^{\left(t\right)}=k\right\},\lambda\right)$
%           \EndFor
%           \State 각 모수를 저장한다.
%         \EndFor
%       \EndProcedure
%       \end{algorithmic}
%     \end{algorithm}

%     위 의사코드(pseudocode)의 5,6,7번째 줄을 해설하자면 이렇게 된다.
%     \begin{enumerate}
%       \item 모든 $N$개의 관측치 $x_{i}$에 새로운 클러스터를 할당한다. $z_{i}^{\left(t\right)}$는 이런 Discrete분포를 따른다.
%       \item 앞서 각 관측치에 새로운 클러스터를 부여했다면, 각 클러스터에 몇개씩 있는지 세서 디리클레 분포의 모수에 더해준다. 그리고 다시 샘플링한다.
%       \item 켤레사전분포를 썼기 때문에 이 사후분포는 closed form을 갖는다. 쉽게 구할 수 있다.
%     \end{enumerate}

%     위의 과정을 보게 되면 각 관측치 $x_{i}$에 새로운 클러스터를 부여해 주려고 할 때마다 $K$개 믹스쳐 컴포넌트에 해당하는 값 $f\left(x_{i}\;|\;\theta_{k}\right)$을 다 계산해야 하기 때문에 시간복잡도가 $\mathcal{O}\left(NK\right)$이다. 이런 연산비용을 줄이기 위해서 Rao-Blackwellized Sampling을 쓸 수 있다. 어렵게 말해서 Rao-Blackwellized Sampling이지 사실은 지수족의 가능도함수가 충분통계량(sufficient statistic)만 알면 업데이트가 가능하다는 데에 착안해서 계산량을 줄이려 한 것 뿐이다. 이에 대해서는 설명을 생략한다.
%   \section{Dirichlet Process Mixtures}
%     앞에서 설명한 FMM은 유한 개의 믹스쳐 컴포넌트가 있다고 가정했다. 하지만 실제 데이터를 관측했을 때는 바로 `아! 몇 개의 클러스터가 섞여있는 거구나!' 알 수 없다(단.호.). 그렇기 때문에 데이터가 알아서 `나 클러스터 몇 개예요!'하고 설명하게 해야 하는데, 이 때 등장하는 게 디리클레 프로세스이다. 데이터가 늘어감에 따라 클러스터도 늘어가게 세팅하는 것이다. 디리클레 프로세스는 근데 Chinese restaurant process에서 설명했듯이 애초에 더 많이 관측된 클러스터가 다시 관측될 확률이 높다. 다시 말해, 스스로 지나치게 많은 클러스터가 생기는 것을 방지하는 내부적인 속성이 있다. 넘나 좋다. \par
%     우리가 이미 믹스쳐 모델을 보았기 때문에 자연스럽게 디리클레 분포 자리에 디리클레 프로세스를 집어넣는 것을 상상하게 되지만, $x_{i}$들이 생성되는 분포를 `모른다'는 것을 비모수적으로 곧바로 모델링하는 사람이라면, $x_{i}$들이 $G\;\sim\;\mathcal{DP}\left(\alpha, H\right)$를 따른다고 해버리고 싶을 것이다. 아니어도 할 수 없다. 그럴 것이다. 그런데 여기서 문제가 발생한다. 일단 첫번째로 디리클레 프로세스의 샘플은 이산형 분포가 된다고 설명한 바 있다. 따라서 여기서 생성된 관측치 $x_{i}$들은 필연적으로 같은 값을 가지는 것들이 발생하고 이산형이 된다. 원래가 연속형이면 잘못된 모델을 적용하는 것이 된다. 이 문제를 해결하기 위해 연속형 분포와 합성을 하게 되면 자연스럽게 Dirichlet process mixture 모델이 된다. (근본적으로 DPM은 hierarchical model이다.)\par
%     FMM에서 보았듯이 이 모델은 다음과 같이 구성된다.
%     \begin{align*}
%       G \;&\sim\; \mathcal{GP}\left(\alpha, H\right)\\
%       \theta_{i}\;&\sim\; G\\
%       x_{i}\;&\sim\;F\left(\theta_{i}\right)
%     \end{align*}
%     여기에 stick-breaking construction을 적용하면
%     \begin{align*}
%       \beta_{k}\;&\sim\; \opn{Beta}\left(\alpha, 1\right)\\
%       \pi_{k}&= \beta_{k}\prod_{\ell=1}^{k-1}\left(1-\beta_{\ell}\right)\\
%       z_{i}\;&\sim\; \opn{Discrete}\left(\bs{\pi}\right)\\
%       \theta_{k}\;&\sim\; H\left(\lambda\right)\\
%       x_{i}\;|\;z_{i}\;&\sim\; F\left(\theta_{z_{i}}\right)
%     \end{align*}
%     $z_{i}$들을 적분해내버리고 나면 FMM의 무한대 꼴로 나타나게 됨을 볼 수 있다.
%     \begin{equation}
%       p\left(x\;|\;\bs{\pi},\theta_{1},\theta_{2},\ldots \right) = \sum_{k=1}^{\infty}\pi_{k}f\left(x\;|\;\theta_{k}\right)
%     \end{equation}
%     요롷게 하면 자동으로 몇 개의 클러스터가 존재하는지 추정해서 알려주는 모델을 만들 수 있고, 깁스 샘플링을 통해 데이터를 학습할 수 있게 된다.
%   \section{Gibbs Sampling for DPM}
%     우선 $N$개의 데이터 $\left(x_{1},\ldots , x_{N}\right)$가 있다고 하자. 위와 같은 DPM을 적용해서 숨어있는 클러스터가 몇 개인지를 알아내고자 한다. 이를 위해 collapsed Gibbs sampler라는 것을 사용해보자. (1.3)을 $z_{i}$에 대해서 쓰면 아래와 같다.
%     \begin{equation}
%       p\left(z_{i}\;|\;z_{\setminus i},\alpha\right)=\frac{1}{\alpha+N-1}\left(\sum_{k=1}^{K}N_{k}^{-i}\delta\left(z_{i},k\right)+\alpha\delta\left(z_{i},k'\right)\right)
%     \end{equation}
%     똑같이 여기서 $k'$는 지금까지 없었던 값이며 새로운 클러스터를 의미한다. 그리고 $x_{i}$가 이미 있는 클러스터에 속해 있다면 가능도 함수는 다음과 같다. (FMM의 경우와 같다.)
%     \begin{equation}
%       p\left(x_{i}\;|\;z_{i}=k, z_{\setminus i}, x_{\setminus i}, \lambda\right) = p\left(x_{i}\;|\; \left\{x_{j}\;|\;z_{j}=k, j \neq i\right\}, \lambda\right)
%     \end{equation}

\begin{thebibliography}{9}
\bibitem{manhattanproject}
\textit{The Master Algorithm: How the Quest for the Ultimate Learning machine Will Remake Our World}, Pedro Domingos, \\\texttt{http://www.amazon.com/The-Master-Algorithm-Ultimate-Learning/dp/0241004543}
\end{thebibliography}
\end{document}


