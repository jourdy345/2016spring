\documentclass[a4paper, 10pt]{book}
\usepackage{titlesec}
\usepackage{kotex}
\usepackage[margin=0.5in]{geometry} % set the margins to 1in on all sides
\usepackage{graphicx} % to include figures
\usepackage{amsmath} % great math stuff
\usepackage{amsfonts} % for blackboard bold, etc
\usepackage{amsthm} % better theorem environments
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{array}
\usepackage{courier}
\usepackage[usenames, dvipsnames]{color}
\usepackage{titlesec}
\usepackage{empheq}
\usepackage{tikz}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}

\linespread{1.5}

\titleformat
{\chapter}
[display]
{\bfseries\Large}
{Chapter \ \thechapter}
{0.5ex}
{
  \rule{\textwidth}{1pt}
  \vspace{1ex}
  \centering
}
[
\vspace{-0.5ex}
\rule{\textwidth}{0.3pt}
]

\titleformat{\section}[wrap]
{\normalfont\bfseries}
{\thesection.}{0.1em}{}

\titlespacing{\section}{11pc}{1.5ex plus .1ex minus .2ex}{1pc}

\newcommand\tikzmark[1]{%
  \tikz[remember picture,overlay]\node[inner sep=2pt] (#1) {};}
\newcommand\DrawBox[3][]{%
  \tikz[remember picture,overlay]\draw[#1] ([xshift=-3.5em,yshift=7pt]#2.north west) rectangle (#3.south east);}

\algnewcommand\algorithmicinput{\textbf{Input:}}
\algnewcommand\INPUT{\item[\algorithmicinput]}
\newcommand\encircle[1]{%
  \tikz[baseline=(X.base)] 
    \node (X) [draw, shape=circle, inner sep=0] {\strut #1};}
 
% Command "alignedbox{}{}" for a box within an align environment
% Source: http://www.latex-community.org/forum/viewtopic.php?f=46&t=8144
\newlength\dlf  % Define a new measure, dlf
\newcommand\alignedbox[2]{
% Argument #1 = before & if there were no box (lhs)
% Argument #2 = after & if there were no box (rhs)
&  % Alignment sign of the line
{
\settowidth\dlf{$\displaystyle #1$}  
    % The width of \dlf is the width of the lhs, with a displaystyle font
\addtolength\dlf{\fboxsep+\fboxrule}  
    % Add to it the distance to the box, and the width of the line of the box
\hspace{-\dlf}  
    % Move everything dlf units to the left, so that & #1 #2 is aligned under #1 & #2
\boxed{#1 #2}
    % Put a box around lhs and rhs
}
}


\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}

\setcounter{secnumdepth}{4}

\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\definecolor{myblue}{RGB}{72, 165, 226}
\definecolor{myorange}{RGB}{222, 141, 8}

\setlength{\heavyrulewidth}{1.5pt}
\setlength{\abovetopsep}{4pt}


\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\argmin}{\arg\!\min}
\DeclareMathOperator{\Tr}{Tr}

\newcommand{\bd}[1]{\mathbf{#1}} % for bolding symbols
\newcommand{\RR}{\mathbb{R}} % for Real numbers
\newcommand{\ZZ}{\mathbb{Z}} % for Integers
\newcommand{\col}[1]{\left[\begin{matrix} #1 \end{matrix} \right]}
\newcommand{\comb}[2]{\binom{#1^2 + #2^2}{#1+#2}}
\newcommand{\bs}{\boldsymbol}
\newcommand{\opn}{\operatorname}
\begin{document}
\nocite{*}

\chapter{Dirichlet process의 정의와 특성}

\section{베이지언 비모수}
  실제로 관측된 데이터셋은 복잡하기 때문에 그것에 맞는 모수적인 모델을 찾기란 때때로 매우 어렵다. \underline{비모수적}인 모델링이란 명시적인 함수의 꼴을 정해주지 않고, 데이터가 관측됨에 따라 모형이 지니는 정확도와 복잡도가 그에 맞게 변할 수 있도록 설계하는 것을 말한다. 언뜻 이름만 봐서는 모수가 없을 것 같지만 실제로는 그 반대이다. 모수가 무한 개로 늘어날 수 있도록 설계한다. 베이지언 통계에서 비모수 모델을 세울 때는 대부분 함수 공간에 분포를 할당한다. 모델을 세울 때는 항상 정확도를 높이되 과하게 복잡해지지 않도록 통제할 필요가 있는데 베이지언 통계에서는 적절한 사전 분포를 할당함으로써 이러한 작업을 한다. 관측된 데이터를 $x_{i}$라 할 때 De Finetti의 정리는 exchangeable한 변수들의 결합분포를 다음과 같이 분해할 수 있다고 증명했다:
  $$
    p\left(x_{1},x_{2},\ldots,x_{N}\right) = \int_{\Theta}p\left(\theta\right)\prod_{i=1}^{N}p\left(x_{i}\middle|\theta\right)\,d\theta.
  $$
  일반적으로 De Finetti의 정리는 모수 공간 $\Theta$가 무한 차원의 확률 측도 공간(probability measure space)일 때만 보장된다. 이 때문에 베이지언 쪽으로 연구하는 사람들은 확률 측도(probability measure)의 분포를 만들어내곤 한다. 그중 Dirichlet process는 우리가 원하는 많은 특성을 지니고 있는 아주 좋은 확률 과정(stochastic process)이다.
\section{확률 측도의 분포}
  확률 과정(stochastic process)들은 무한 차원의 공간을 모델링하는 데 많이 쓰이고, 이를 위해 만들어지기도 한다. 베이지언 비모수 역시 무한 차원의 공간을 모델링하기 위해 확률 과정을 많이 쓰는데, 사실 무한 차원은 이해하기 어렵기 때문에 이 과정(process)들을 정의할 때 유한 개만 가져오면 무엇이 되는지로 정의할 때가 많다. 대표적인 것이 Gaussian process이다. Gaussian process는 주로 비선형 회귀 모형에서 함수의 꼴을 모르기 때문에 무한 차원의 함수 공간을 모델링하기 위해 쓰이는데 그 정의는 다음과 같다. 어떤 함수 $f\; : \; \mathcal{X}\mapsto \mathbb{R}$가 Gaussian process를 따른다 함은 $N$개의 $x_{i}\in \mathcal{X}$를 임의적으로 가져왔을 때 이들의 결합 분포 $p\left(f\left(x_{1}\right),\ldots ,f\left(x_{N}\right)\right)$가 정규분포를 따른다는 말과 동치이다. 이렇게 정의하게 되면 Gaussian process는 적절한 평균함수(mean function)과 공분산 함수(covariance function)로 정의될 수 있고, 이를 사용하였을 때 추정을 위한 계산이 쉬워진다. \par
  Gaussian process가 알지 못하는 함수를 모델링할 때 쓰인다면 Dirichlet process는 알지 못하는 확률 측도(probability measure) 혹은 적분했을 때 값이 1이 되는 0이 아닌 함수들의 분포를 모델링할 때 쓰인다. 모수 공간을 $\Theta$라고 했을 때 Dirichlet process는 $\Theta$에 대한 기저 측도(base measure) $H$와 집적 모수(concentration parameter) $\alpha$로 정의할 수 있다. 가우시안 프로세스가 임의로 뽑은 $N$개의 유한한 데이터가 어떤 분포를 가지는지로 정의되었던 것처럼, 디리클레 프로세스 역시 모수 공간 $\Theta$의 분할(partition)에 대해서 기저 측도가 어떤 분포를 만들어내는지로 정의할 수 있다.
  \begin{thm}
    $H$가 측정가능한 공간(measurable space) $\Theta$에 대한 확률 분포이고 $\alpha$가 어떤 양의 상수일 때, 모수 공간 $\Theta$의 유한한 분할 $\left(A_{1},\ldots , A_{N}\right)$에 대해 다음이 성립한다.
    $$
      \bigcup_{i=1}^{N}A_{i} = \Theta \hspace{35pt} A_{i}\cap A_{j} = \emptyset,\; i \neq j
    $$
    우리는 모수 공간 $\Theta$에 대한 임의의 확률 분포 $G$가 디리클레 프로세스를 따른다고 함은 위와 같은 성질을 만족하는 모든 유한한 분할에 대한 측도가 다음과 같이 디리클레 분포를 따른다는 말과 동치이다.
    $$
      \left(G\left(A_{1}\right),\ldots ,G\left(A_{N}\right)\right) \;\sim\; \opn{Dir}\left(\alpha H\left(A_{1}\right),\ldots , \alpha H\left(A_{N}\right)\right)
    $$
    어떠한 기저측도 $H$와 집적 모수 $\alpha$라도 위와 같은 성질은 만족하는 유일한 확률과정이 존재하는데 이를 우리는 $\mathcal{DP}\left(\alpha, H\right)$라고 표기한다.
  \end{thm}
  이러한 확률 과정이 존재한다는 사실을 퍼거슨(Ferguson)이 콜모고로프(Kolmogorov)의 일치성 조건(consistency condition)을 이용하여 밝혀냈는데, 훗날 세스라만(Sethuraman)이 더 간단한 정의로 쉽게 똑같은 것을 증명해내는 데 성공한다.
\section{디리클레 프로세스의 성질}
  앞서 설명한 수학적인 디리클레 프로세스의 정의는 측도 이론(measure theory)를 이해하고 있으면 더 편하게 다가오지만 그렇지 않은 사람에게는 좀 어렵다. 어느 정도의 측도 이론은 공부할 필요가 있는 듯싶다. 하지만 최대한 쉽고 직관적으로 설명해 보려고 노력하겠다. 아무튼 어떠한 확률 분포에서 임의의 확률 변수를 추출하면 그 확률 분포가 지니는 정의역에서 하나의 값이 튀어나온다. 이를 샘플링한다고 말하는데, 예를 들어 포아송 분포에서 샘플링하게 되면 음이 아닌 정수들이 튀어나오게 되고, 각 값이 튀어나오는 빈도는 포아송 분포가 지니는 확률값에 의존한다. 또 정규분포에서 샘플링하면 실수 수직선 상에서 값들이 나오게 되는데 그 빈도 역시 평균에 가까운 값은 많이 나오는 형태로 나오게 된다. 즉 밀도함수와 비슷하게 정의역 값이 추출된느 것이다. 디리클레 분포는 일명 `분포의 분포'로 이로부터 샘플링하게 되면 하나의 분포가 나오는 것이다. 어떤 모수 공간의 부분집합 $A \subset \Theta$에 대해 디리클레 분포에서 추출된 샘플이 할당하는 값의 평균은 다음과 같이 쓸 수 있다.
  $$
  \mathbb{E}\left[G\left(A\right)\right]=H\left(A\right) \hspace{35pt} G\sim \mathcal{DP}\left(\alpha, H\right).
  $$
  이 때문에 기저 측도라 부르고 있는 $H$는 디리클레 분포의 평균이 된다. 이와 달리 집적 모수 $\alpha$는 산포도(퍼진 정도)를 결정하게 되는데 디리클레 분포의 정밀도(precision)과 비슷하다고 생각하면 된다. \par
  이제 사후 분포에 대해 이야기해보자. 앞서 말했듯이 디리클레 프로세스에서 샘플을 추출하게 되면 그것은 임의의 확률 분포가 된다. 그럼 일반적으로 했듯이 이 확률 분포에서 또다시 확률 변수를 샘플링할 수 있다. 다음과 같은 구조이다.
  \begin{align*}
    G &\sim \mathcal{DP}\left(\alpha, H\right)\\
    \theta\;|\; G &\sim G
  \end{align*}
  잠깐 일반적인 정규분포 표본의 문제로 돌아가보자. 문제가 다음과 같을 때 모수의 사후 분포를 계산하는 것을 다시 한 번 생각해보자.
  \begin{align*}
    &\mu \;\sim\; \mathcal{N}\left(\mu_{0},\sigma_{0}^{2}\right)\\
    &X_{1},X_{2},\ldots , X_{N}\;|\;\mu \sim \mathcal{N}\left(\mu,\sigma^{2}\right)
  \end{align*}
  이럴 때 우리는 평균 $\mu$의 사후 분포 $\mu\;|\;X_{1},\ldots X_{N}$를 구할 수 있었다. 동일하게 확률 분포 $G$의 사후분포를 구할 수 있을 것이란 생각은 굉장히 자연스럽다. 이는 디리클레 분포의 켤레성(conjugacy) 덕에 쉽게 구할 수 있다. 예를 들어, $\theta$가 모수 공간을 분할한 것 중 하나에 속한다면 디리클레 분포의 모수 중 똑같은 부분에 하나를 더해주면 된다.
  $$
  \left(G\left(A_{1}\right),\ldots ,G\left(A_{N}\right)\;|\;\theta\in A_{n} \right) \;\sim\; \opn{Dir}\left(\alpha H\left(A_{1}\right),\ldots , \alpha H\left(A_{n}\right), \ldots , \alpha H\left(A_{N}\right)\right),\quad n\in\left\{1,\ldots , N\right\}
  $$
  이로부터 자연스럽게 $G$의 사후분포를 유도해낼 수 있는데 그것은 다음과 같이 정리할 수 있다.
  \begin{thm}
    $G \sim \mathcal{DP}\left(\alpha, H\right)$가 임의의 확률 분포라고 하자. 그리고 이로부터 $N$개의 표본 $\theta_{i} \;\sim\; G$을 관측하였을 때 사후 분포 역시 업데이트된 디리클레 프로세스가 된다.
    $$
      G\;|\;\theta_{1},\ldots ,\theta_{N},\alpha,H \;\sim\; \mathcal{DP}\left(\alpha+ N, \frac{1}{\alpha + N}\left(\alpha H+\sum_{i=1}^{N}\delta_{\theta_{i}}\right)\right)
    $$
  \end{thm}
\section{Stick-Breaking process}
  앞장의 모든 설명은 수학적으로는 증명이 되고 유용하다는 것이 입증되기는 했지만 실제로 디리클레 분포로부터 어떻게 샘플을 추출할 수 있는지 그 매커니즘은 제공하지 않는다. 세스라만(Sethuraman---사실 어떻게 음역해야 할지 참 곤란하다---)은 stick-breaking construction이라는 이름으로 이를 해결하고 더 나아가 예측 분포가 어떻게 생기는지에 대한 프로세스인 Chinese restaurant process를 고안하는 토대가 된다. 디리클레 프로세스에서 추출된 분포는 모두 이산형이다. 이를 염두해 두고 앞에서 구한 사후 분포를 다시 생각해보자. 이에 따르면 똑같이 모수 공간의 어떤 부분집합 $A$에 대해 사후 분포는 다음과 같은 평균을 갖는다.
  \begin{equation}
    \mathbb{E}\left[G\left(A\right)\;|\;\theta_{1},\ldots , \theta_{N},\alpha, H\right] = \frac{1}{\alpha+H}\left(\alpha H\left(A\right)+\sum_{i=1}^{N}\delta_{\theta_{i}\left(A\right)}\right)
  \end{equation}
  여기에서 $N\to\infty$하게 되면 평균은 $\sum_{k=1}^{\infty}\pi_{k}\delta_{\theta_{k}}\left(A\right)$가 된다. 여기서 $\pi_{k}$는 무한 개의 관측치 $\theta_{1},\theta_{2},\ldots$에서 $\theta_{k}$의 상대 빈도를 나타낸다. 그러니까 무한 개에서 특정한 관측치의 개수를 세서 전체에서의 상대 빈도를 계산한다는 것은 그 비율이 어떤 수로 수렴한다는 것을 의미한다. 이는 디리클레 프로세스에서 표본이 되는 확률 분포를 추출하는 메커니즘을 제공한다.
  \begin{thm}
    $G\;\sim\; \mathcal{DP}\left(\alpha, H\right)$라고 할 때, 다음과 같이 재현될 수 있다.
    \begin{align*}
      &\beta_{k}\;\sim\;\opn{Beta}\left(1,\alpha\right)\hspace{100pt} k=1,2,\ldots\\
      &\pi_{k} = \beta_{k}\prod_{\ell=1}^{k-1}\left(1-\beta_{\ell}\right) = \beta_{k}\left(1-\sum_{\ell=1}^{k-1}\pi_{\ell}\right)\\
      &G\;=\;\sum_{k=1}^{\infty}\pi_{k}\delta_{\theta_{k}}\hspace{111pt} \theta_{k}\;\sim\; H
    \end{align*}
  \end{thm}
  
  베타 분포의 샘플과 기저 측도에서 추출한 샘플을 적당히 섞어주면 그것의 무한합은 디리클레 프로세스를 따르게 된다. 이를 통해 실질적으로 컴퓨터로 디리클레 프로세스를 구현할 수 있게 되었다. 이름이 의미하는 바가 큰데 이것을 stick-breaking process라고 명명한 데에는 꽤나 직관적인 해석이 숨어있다. 베타 분포에서 처음 $\beta_{1}$을 추출했을 때는 0과 1 사이의 어떤 사이를 뽑음으로써 길이가 1인 막대기를 2개로 부러뜨린 것과 같다. 그리고 $\pi_{k}$를 정의한 것을 자세히 보면 $\beta_{1}$을 뽑고 남은 부분에 대해서---$\left(1-\beta_{1}\right)$---다시 $\beta_{2}$의 비율만큼 부러뜨리고, 또 남은 것을 $\beta_{3}$의 비율로 부러뜨리고 이 과정을 반복한다. 마지막에 $\pi_{k}\delta_{\theta_{k}}$의 의미는 실수 축에서 $\theta_{k}$자리에 $\pi_{k}$만큼 확률값을 주자는 말이다. 이를 흔히 `확률질량'이라고 부른다. 이제 예측 분포를 어떻게 계산할 수 있는지에 대해 이야기해보도록 하자.
\section{폴리아 항아리\\(Pólya Urns)}
  디리클레 프로세스의 표본은 이산형 분포가 되기 때문에 필연적으로 이 분포의 샘플은 같은 값을 가지는 관측치가 생기게 된다. 따라서 $N$개의 관측치 $\theta_{1},\ldots , \theta_{N}$를 관측했다 하더라도 이들이 지니는 값은 모두 다르면 $N$개, 겹치는 게 있으면 $N$보다 작아지게 된다. 따라서 이 서로 다른 값을 $\left\{\theta_{k}'\right\}_{k=1}^{K},\; K\leq N$라고 쓰면 (1.1)을 아래와 같이 다시 쓸 수 있게 된다.
  $$
    \mathbb{E}\left[G\left(A\right)\;|\;\theta_{1},\ldots , \theta_{N},\alpha, H\right] = \frac{1}{\alpha+N}\left(\alpha H\left(A\right)+\sum_{k=1}^{K}N_{k}\delta_{\theta_{k}'}\left(A\right)\right)
  $$
  여기서 $N_{k}$는 $\theta_{k}'$의 값을 지니는 관측치의 개수이다.
  \begin{thm}
    $G\;\sim\;\mathcal{DP}\left(\alpha, H\right)$에서 기저 측도 $H$가 밀도함수 $h\left(\theta\right)$를 가지고, $N$개의 관측치 $\theta_{i}\;\sim\; G$가 $K$개의 서로 다른 값 $\left\{\theta_{k}'\right\}_{k=1}^{K}$를 가진다고 하자. 다음 관측치의 예측 분포는 다음과 같다.
    $$
      p\left(\theta_{N+1}=\theta' \;|\; \theta_{1},\ldots , \theta_{N},\alpha, H\right)=\frac{1}{\alpha+N}\left(\alpha h\left(\theta'\right)+\sum_{k=1}^{K}N_{k}\delta\left(\theta',\theta'_{k}\right)\right)
    $$
    여기서 $N_{k}$ $\theta_{1},\ldots , \theta_{N}$에서 값 $\theta_{k}'$을 지니는 것들의 개수이다.
  \end{thm}
  
  이러한 예측모형은 폴리아 항아리 모델의 일반화된 버전이라고 볼 수 있다. 하나의 항아리에는 그전까지의 관측치의 개수만큼 공이 있는데, 서로 다른 값은 서로 다른 색깔의 공으로 표시한다. 그러니까 서로 다른 색깔의 공이 그 값을 지니는 관측치 개수만큼 있다고 생각하면 된다. 그리고 마지막으로 $\frac{\alpha}{\alpha+N}$의 확률로 뽑히는 공이 하나 더 있는데 이 공은 뽑히는 순간 색깔이 결정되고, 그 색깔은 여태 보지 못한 색일 수 있다. 새로운 값을 지니는 관측치가 저 확률로 뽑히게 된다는 뜻이다. 이 이야기를 이용하면 명시적으로 $G\;\sim\;\mathcal{DP}\left(\alpha, H\right)$를 만들지 않고도 관측치들을 샘플링할 수 있다는 장점이 있다.
  \section{Chinese Restaurant Process}

\end{document}


